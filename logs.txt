
==> Audit <==
|----------------|--------------------------------|----------|-------|---------|---------------------|---------------------|
|    Command     |              Args              | Profile  | User  | Version |     Start Time      |      End Time       |
|----------------|--------------------------------|----------|-------|---------|---------------------|---------------------|
| start          |                                | minikube | apple | v1.35.0 | 28 Mar 25 11:43 PDT | 28 Mar 25 11:44 PDT |
| start          |                                | minikube | apple | v1.35.0 | 28 Mar 25 23:57 PDT | 28 Mar 25 23:58 PDT |
| start          |                                | minikube | apple | v1.35.0 | 29 Mar 25 06:47 PDT | 29 Mar 25 06:48 PDT |
| start          |                                | minikube | apple | v1.35.0 | 29 Mar 25 08:44 PDT | 29 Mar 25 08:45 PDT |
| start          |                                | minikube | apple | v1.35.0 | 29 Mar 25 23:17 PDT | 29 Mar 25 23:18 PDT |
| start          |                                | minikube | apple | v1.35.0 | 30 Mar 25 06:02 PDT |                     |
| start          |                                | minikube | apple | v1.35.0 | 30 Mar 25 06:05 PDT | 30 Mar 25 06:07 PDT |
| start          |                                | minikube | apple | v1.35.0 | 01 Apr 25 02:16 PDT | 01 Apr 25 02:17 PDT |
| start          |                                | minikube | apple | v1.35.0 | 01 Apr 25 03:17 PDT | 01 Apr 25 03:18 PDT |
| start          |                                | minikube | apple | v1.35.0 | 01 Apr 25 07:35 PDT | 01 Apr 25 07:36 PDT |
| start          |                                | minikube | apple | v1.35.0 | 03 Apr 25 03:07 PDT | 03 Apr 25 03:08 PDT |
| start          |                                | minikube | apple | v1.35.0 | 05 Apr 25 10:08 PDT | 05 Apr 25 10:10 PDT |
| start          |                                | minikube | apple | v1.35.0 | 07 Apr 25 09:07 PDT | 07 Apr 25 09:08 PDT |
| start          |                                | minikube | apple | v1.35.0 | 07 Apr 25 23:53 PDT | 07 Apr 25 23:54 PDT |
| start          |                                | minikube | apple | v1.35.0 | 08 Apr 25 23:49 PDT | 08 Apr 25 23:50 PDT |
| start          |                                | minikube | apple | v1.35.0 | 09 Apr 25 01:37 PDT | 09 Apr 25 01:38 PDT |
| start          |                                | minikube | apple | v1.35.0 | 09 Apr 25 11:18 PDT |                     |
| start          |                                | minikube | apple | v1.35.0 | 10 Apr 25 09:15 PDT | 10 Apr 25 09:16 PDT |
| start          |                                | minikube | apple | v1.35.0 | 11 Apr 25 04:40 PDT | 11 Apr 25 04:41 PDT |
| start          |                                | minikube | apple | v1.35.0 | 11 Apr 25 11:04 PDT | 11 Apr 25 11:05 PDT |
| start          |                                | minikube | apple | v1.35.0 | 12 Apr 25 00:26 PDT | 12 Apr 25 00:27 PDT |
| start          |                                | minikube | apple | v1.35.0 | 12 Apr 25 07:50 PDT | 12 Apr 25 07:51 PDT |
| start          |                                | minikube | apple | v1.35.0 | 12 Apr 25 11:22 PDT | 12 Apr 25 11:23 PDT |
| update-context |                                | minikube | apple | v1.35.0 | 12 Apr 25 16:19 PDT | 12 Apr 25 16:19 PDT |
| stop           |                                | minikube | apple | v1.35.0 | 12 Apr 25 16:19 PDT | 12 Apr 25 16:20 PDT |
| start          |                                | minikube | apple | v1.35.0 | 12 Apr 25 16:20 PDT | 12 Apr 25 16:22 PDT |
| stop           |                                | minikube | apple | v1.35.0 | 12 Apr 25 16:44 PDT | 12 Apr 25 16:44 PDT |
| start          |                                | minikube | apple | v1.35.0 | 12 Apr 25 16:44 PDT | 12 Apr 25 16:46 PDT |
| start          |                                | minikube | apple | v1.35.0 | 13 Apr 25 05:03 PDT | 13 Apr 25 05:04 PDT |
| start          |                                | minikube | apple | v1.35.0 | 15 Apr 25 06:37 PDT | 15 Apr 25 06:38 PDT |
| start          |                                | minikube | apple | v1.35.0 | 15 Apr 25 11:19 PDT | 15 Apr 25 11:20 PDT |
| start          |                                | minikube | apple | v1.35.0 | 16 Apr 25 08:19 PDT | 16 Apr 25 08:20 PDT |
| start          |                                | minikube | apple | v1.35.0 | 17 Apr 25 02:02 PDT | 17 Apr 25 02:02 PDT |
| start          |                                | minikube | apple | v1.35.0 | 17 Apr 25 10:24 PDT | 17 Apr 25 10:26 PDT |
| start          |                                | minikube | apple | v1.35.0 | 18 Apr 25 01:46 PDT | 18 Apr 25 01:47 PDT |
| start          |                                | minikube | apple | v1.35.0 | 19 Apr 25 16:11 PDT | 19 Apr 25 16:13 PDT |
| start          |                                | minikube | apple | v1.35.0 | 24 Apr 25 07:49 PDT | 24 Apr 25 07:50 PDT |
| start          |                                | minikube | apple | v1.35.0 | 29 Apr 25 00:32 PDT | 29 Apr 25 00:32 PDT |
| start          |                                | minikube | apple | v1.35.0 | 01 May 25 03:52 PDT | 01 May 25 03:52 PDT |
| start          |                                | minikube | apple | v1.35.0 | 01 May 25 22:56 PDT | 01 May 25 22:57 PDT |
| start          |                                | minikube | apple | v1.35.0 | 02 May 25 13:03 PDT | 02 May 25 13:04 PDT |
| start          |                                | minikube | apple | v1.35.0 | 06 May 25 00:10 PDT |                     |
| start          |                                | minikube | apple | v1.35.0 | 06 May 25 00:13 PDT |                     |
| stop           |                                | minikube | apple | v1.35.0 | 06 May 25 00:14 PDT | 06 May 25 00:14 PDT |
| start          |                                | minikube | apple | v1.35.0 | 06 May 25 00:15 PDT | 06 May 25 00:16 PDT |
| tunnel         |                                | minikube | apple | v1.35.0 | 06 May 25 05:01 PDT | 06 May 25 06:55 PDT |
| addons         | enable ingress                 | minikube | apple | v1.35.0 | 06 May 25 06:59 PDT | 06 May 25 07:02 PDT |
| service        | nginx-service                  | minikube | apple | v1.35.0 | 06 May 25 09:30 PDT | 06 May 25 09:30 PDT |
| service        | nginx-service                  | minikube | apple | v1.35.0 | 06 May 25 09:47 PDT | 06 May 25 09:47 PDT |
| ip             |                                | minikube | apple | v1.35.0 | 06 May 25 09:58 PDT | 06 May 25 09:58 PDT |
| start          |                                | minikube | apple | v1.35.0 | 07 May 25 14:28 PDT | 07 May 25 14:29 PDT |
| start          |                                | minikube | apple | v1.35.0 | 11 May 25 15:25 PDT | 11 May 25 15:26 PDT |
| start          |                                | minikube | apple | v1.35.0 | 12 May 25 00:50 PDT | 12 May 25 00:51 PDT |
| ip             |                                | minikube | apple | v1.35.0 | 12 May 25 02:11 PDT | 12 May 25 02:11 PDT |
| service        | my-app-service                 | minikube | apple | v1.35.0 | 12 May 25 02:22 PDT | 12 May 25 02:22 PDT |
| service        | nginx                          | minikube | apple | v1.35.0 | 12 May 25 02:27 PDT |                     |
| tunnel         |                                | minikube | apple | v1.35.0 | 12 May 25 08:11 PDT | 12 May 25 08:12 PDT |
| service        | prometheus-server -n           | minikube | apple | v1.35.0 | 12 May 25 08:21 PDT | 12 May 25 08:21 PDT |
|                | monitoring                     |          |       |         |                     |                     |
| service        | grafana -n monitoring          | minikube | apple | v1.35.0 | 12 May 25 09:14 PDT |                     |
| service        | grafana -n monitoring          | minikube | apple | v1.35.0 | 12 May 25 09:19 PDT |                     |
|----------------|--------------------------------|----------|-------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/05/12 00:50:58
Running on machine: obogozyspifApples-MacBook-Pro
Binary: Built with gc go1.23.4 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0512 00:50:58.321861    1435 out.go:345] Setting OutFile to fd 1 ...
I0512 00:50:58.322214    1435 out.go:397] isatty.IsTerminal(1) = true
I0512 00:50:58.322219    1435 out.go:358] Setting ErrFile to fd 2...
I0512 00:50:58.322226    1435 out.go:397] isatty.IsTerminal(2) = true
I0512 00:50:58.322440    1435 root.go:338] Updating PATH: /Users/apple/.minikube/bin
I0512 00:50:58.325879    1435 out.go:352] Setting JSON to false
I0512 00:50:58.362126    1435 start.go:129] hostinfo: {"hostname":"obogozyspifApples-MacBook-Pro.local","uptime":4404,"bootTime":1747031854,"procs":495,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"13.6.1","kernelVersion":"22.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"545e4134-f741-5de5-b0b9-1729209d14ab"}
W0512 00:50:58.362900    1435 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0512 00:50:58.376260    1435 out.go:177] üòÑ  minikube v1.35.0 on Darwin 13.6.1
I0512 00:50:58.400836    1435 notify.go:220] Checking for updates...
I0512 00:50:58.401847    1435 config.go:182] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0512 00:50:58.402949    1435 driver.go:394] Setting default libvirt URI to qemu:///system
I0512 00:50:58.403898    1435 main.go:141] libmachine: Found binary path at /Users/apple/.minikube/bin/docker-machine-driver-hyperkit
I0512 00:50:58.403969    1435 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0512 00:50:58.420702    1435 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:50200
I0512 00:50:58.421175    1435 main.go:141] libmachine: () Calling .GetVersion
I0512 00:50:58.421659    1435 main.go:141] libmachine: Using API Version  1
I0512 00:50:58.421668    1435 main.go:141] libmachine: () Calling .SetConfigRaw
I0512 00:50:58.421928    1435 main.go:141] libmachine: () Calling .GetMachineName
I0512 00:50:58.422051    1435 main.go:141] libmachine: (minikube) Calling .DriverName
I0512 00:50:58.503898    1435 out.go:177] ‚ú®  Using the hyperkit driver based on existing profile
I0512 00:50:58.528930    1435 start.go:297] selected driver: hyperkit
I0512 00:50:58.528956    1435 start.go:901] validating driver "hyperkit" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.64.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0512 00:50:58.529826    1435 start.go:912] status for hyperkit: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0512 00:50:58.530347    1435 install.go:52] acquiring lock: {Name:mk4023283b30b374c3f04c8805d539e68824c0b8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0512 00:50:58.530802    1435 install.go:117] Validating docker-machine-driver-hyperkit, PATH=/Users/apple/.minikube/bin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/apple/Library/Application Support/Code/User/globalStorage/github.copilot-chat/debugCommand
I0512 00:50:58.549168    1435 install.go:137] /Users/apple/.minikube/bin/docker-machine-driver-hyperkit version is 1.35.0
I0512 00:50:58.558753    1435 install.go:79] stdout: /Users/apple/.minikube/bin/docker-machine-driver-hyperkit
I0512 00:50:58.558781    1435 install.go:81] /Users/apple/.minikube/bin/docker-machine-driver-hyperkit looks good
I0512 00:50:58.559770    1435 cni.go:84] Creating CNI manager for ""
I0512 00:50:58.560221    1435 cni.go:158] "hyperkit" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0512 00:50:58.560326    1435 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.64.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0512 00:50:58.560483    1435 iso.go:125] acquiring lock: {Name:mkbe76d476d9e1b2a1a7055c79b38815db8ae6cd Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0512 00:50:58.583301    1435 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0512 00:50:58.602060    1435 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0512 00:50:58.602197    1435 preload.go:146] Found local preload: /Users/apple/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0512 00:50:58.602230    1435 cache.go:56] Caching tarball of preloaded images
I0512 00:50:58.604231    1435 preload.go:172] Found /Users/apple/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0512 00:50:58.604349    1435 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0512 00:50:58.604635    1435 profile.go:143] Saving config to /Users/apple/.minikube/profiles/minikube/config.json ...
I0512 00:50:58.610629    1435 start.go:360] acquireMachinesLock for minikube: {Name:mkad6730d46f9d1b8c3bc1adcf915e7a8611056e Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0512 00:50:58.610921    1435 start.go:364] duration metric: took 267.256¬µs to acquireMachinesLock for "minikube"
I0512 00:50:58.610966    1435 start.go:96] Skipping create...Using existing machine configuration
I0512 00:50:58.610985    1435 fix.go:54] fixHost starting: 
I0512 00:50:58.611593    1435 main.go:141] libmachine: Found binary path at /Users/apple/.minikube/bin/docker-machine-driver-hyperkit
I0512 00:50:58.612468    1435 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0512 00:50:58.626579    1435 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:50202
I0512 00:50:58.627165    1435 main.go:141] libmachine: () Calling .GetVersion
I0512 00:50:58.627630    1435 main.go:141] libmachine: Using API Version  1
I0512 00:50:58.627639    1435 main.go:141] libmachine: () Calling .SetConfigRaw
I0512 00:50:58.627944    1435 main.go:141] libmachine: () Calling .GetMachineName
I0512 00:50:58.628101    1435 main.go:141] libmachine: (minikube) Calling .DriverName
I0512 00:50:58.628236    1435 main.go:141] libmachine: (minikube) Calling .GetState
I0512 00:50:58.628345    1435 main.go:141] libmachine: (minikube) DBG | exe=/Users/apple/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0512 00:50:58.628507    1435 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 941
I0512 00:50:58.630270    1435 main.go:141] libmachine: (minikube) DBG | hyperkit pid 941 missing from process table
I0512 00:50:58.630290    1435 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
I0512 00:50:58.630310    1435 main.go:141] libmachine: (minikube) Calling .DriverName
W0512 00:50:58.630433    1435 fix.go:138] unexpected machine state, will restart: <nil>
I0512 00:50:58.639385    1435 out.go:177] üîÑ  Restarting existing hyperkit VM for "minikube" ...
I0512 00:50:58.650564    1435 main.go:141] libmachine: (minikube) Calling .Start
I0512 00:50:58.650877    1435 main.go:141] libmachine: (minikube) DBG | exe=/Users/apple/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0512 00:50:58.653557    1435 main.go:141] libmachine: (minikube) minikube might have been shutdown in an unclean way, the hyperkit pid file still exists: /Users/apple/.minikube/machines/minikube/hyperkit.pid
I0512 00:50:58.655629    1435 main.go:141] libmachine: (minikube) DBG | hyperkit pid 941 missing from process table
I0512 00:50:58.655654    1435 main.go:141] libmachine: (minikube) DBG | pid 941 is in state "Stopped"
I0512 00:50:58.655688    1435 main.go:141] libmachine: (minikube) DBG | Removing stale pid file /Users/apple/.minikube/machines/minikube/hyperkit.pid...
I0512 00:50:58.658836    1435 main.go:141] libmachine: (minikube) DBG | Using UUID ed4022c9-f055-4ca4-9fcb-89a08dc27b56
I0512 00:50:59.291301    1435 main.go:141] libmachine: (minikube) DBG | Generated MAC be:16:53:c0:e0:21
I0512 00:50:59.291338    1435 main.go:141] libmachine: (minikube) DBG | Starting with cmdline: loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube
I0512 00:50:59.292590    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:50:59 DEBUG: hyperkit: Start &hyperkit.HyperKit{HyperKit:"/usr/local/bin/hyperkit", Argv0:"", StateDir:"/Users/apple/.minikube/machines/minikube", VPNKitSock:"", VPNKitUUID:"", VPNKitPreferredIPv4:"", UUID:"ed4022c9-f055-4ca4-9fcb-89a08dc27b56", Disks:[]hyperkit.Disk{(*hyperkit.RawDisk)(0xc0003fab70)}, ISOImages:[]string{"/Users/apple/.minikube/machines/minikube/boot2docker.iso"}, VSock:false, VSockDir:"", VSockPorts:[]int(nil), VSockGuestCID:3, VMNet:true, Sockets9P:[]hyperkit.Socket9P(nil), Kernel:"/Users/apple/.minikube/machines/minikube/bzimage", Initrd:"/Users/apple/.minikube/machines/minikube/initrd", Bootrom:"", CPUs:2, Memory:4000, Console:1, Serials:[]hyperkit.Serial(nil), Pid:0, Arguments:[]string(nil), CmdLine:"", process:(*os.Process)(nil)}
I0512 00:50:59.292666    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:50:59 DEBUG: hyperkit: check &hyperkit.HyperKit{HyperKit:"/usr/local/bin/hyperkit", Argv0:"", StateDir:"/Users/apple/.minikube/machines/minikube", VPNKitSock:"", VPNKitUUID:"", VPNKitPreferredIPv4:"", UUID:"ed4022c9-f055-4ca4-9fcb-89a08dc27b56", Disks:[]hyperkit.Disk{(*hyperkit.RawDisk)(0xc0003fab70)}, ISOImages:[]string{"/Users/apple/.minikube/machines/minikube/boot2docker.iso"}, VSock:false, VSockDir:"", VSockPorts:[]int(nil), VSockGuestCID:3, VMNet:true, Sockets9P:[]hyperkit.Socket9P(nil), Kernel:"/Users/apple/.minikube/machines/minikube/bzimage", Initrd:"/Users/apple/.minikube/machines/minikube/initrd", Bootrom:"", CPUs:2, Memory:4000, Console:1, Serials:[]hyperkit.Serial(nil), Pid:0, Arguments:[]string(nil), CmdLine:"", process:(*os.Process)(nil)}
I0512 00:50:59.293453    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:50:59 DEBUG: hyperkit: Arguments: []string{"-A", "-u", "-F", "/Users/apple/.minikube/machines/minikube/hyperkit.pid", "-c", "2", "-m", "4000M", "-s", "0:0,hostbridge", "-s", "31,lpc", "-s", "1:0,virtio-net", "-U", "ed4022c9-f055-4ca4-9fcb-89a08dc27b56", "-s", "2:0,virtio-blk,/Users/apple/.minikube/machines/minikube/minikube.rawdisk", "-s", "3,ahci-cd,/Users/apple/.minikube/machines/minikube/boot2docker.iso", "-s", "4,virtio-rnd", "-l", "com1,autopty=/Users/apple/.minikube/machines/minikube/tty,log=/Users/apple/.minikube/machines/minikube/console-ring", "-f", "kexec,/Users/apple/.minikube/machines/minikube/bzimage,/Users/apple/.minikube/machines/minikube/initrd,earlyprintk=serial loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube"}
I0512 00:50:59.293501    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:50:59 DEBUG: hyperkit: CmdLine: "/usr/local/bin/hyperkit -A -u -F /Users/apple/.minikube/machines/minikube/hyperkit.pid -c 2 -m 4000M -s 0:0,hostbridge -s 31,lpc -s 1:0,virtio-net -U ed4022c9-f055-4ca4-9fcb-89a08dc27b56 -s 2:0,virtio-blk,/Users/apple/.minikube/machines/minikube/minikube.rawdisk -s 3,ahci-cd,/Users/apple/.minikube/machines/minikube/boot2docker.iso -s 4,virtio-rnd -l com1,autopty=/Users/apple/.minikube/machines/minikube/tty,log=/Users/apple/.minikube/machines/minikube/console-ring -f kexec,/Users/apple/.minikube/machines/minikube/bzimage,/Users/apple/.minikube/machines/minikube/initrd,earlyprintk=serial loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube"
I0512 00:50:59.293530    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:50:59 DEBUG: hyperkit: Redirecting stdout/stderr to logger
I0512 00:50:59.298281    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:50:59 DEBUG: hyperkit: Pid is 1449
I0512 00:50:59.299494    1435 main.go:141] libmachine: (minikube) DBG | Attempt 0
I0512 00:50:59.299506    1435 main.go:141] libmachine: (minikube) DBG | exe=/Users/apple/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0512 00:50:59.299521    1435 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1449
I0512 00:50:59.308065    1435 main.go:141] libmachine: (minikube) DBG | Searching for be:16:53:c0:e0:21 in /var/db/dhcpd_leases ...
I0512 00:50:59.308077    1435 main.go:141] libmachine: (minikube) DBG | Found 1 entries in /var/db/dhcpd_leases!
I0512 00:50:59.308096    1435 main.go:141] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.2 HWAddress:be:16:53:c0:e0:21 ID:1,be:16:53:c0:e0:21 Lease:0x68227562}
I0512 00:50:59.308101    1435 main.go:141] libmachine: (minikube) DBG | Found match: be:16:53:c0:e0:21
I0512 00:50:59.308110    1435 main.go:141] libmachine: (minikube) DBG | IP: 192.168.64.2
I0512 00:50:59.308192    1435 main.go:141] libmachine: (minikube) Calling .GetConfigRaw
I0512 00:50:59.309453    1435 main.go:141] libmachine: (minikube) Calling .GetIP
I0512 00:50:59.309863    1435 profile.go:143] Saving config to /Users/apple/.minikube/profiles/minikube/config.json ...
I0512 00:50:59.310680    1435 machine.go:93] provisionDockerMachine start ...
I0512 00:50:59.310732    1435 main.go:141] libmachine: (minikube) Calling .DriverName
I0512 00:50:59.311372    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:50:59.312137    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:50:59.312356    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:50:59.312523    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:50:59.312752    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:50:59.313396    1435 main.go:141] libmachine: Using SSH client type: native
I0512 00:50:59.314451    1435 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1097816a0] 0x109784380 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I0512 00:50:59.314459    1435 main.go:141] libmachine: About to run SSH command:
hostname
I0512 00:50:59.330593    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:50:59 INFO : hyperkit: stderr: Using fd 5 for I/O notifications
I0512 00:50:59.479759    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:50:59 INFO : hyperkit: stderr: /Users/apple/.minikube/machines/minikube/boot2docker.iso: fcntl(F_PUNCHHOLE) Operation not permitted: block device will not support TRIM/DISCARD
I0512 00:50:59.481815    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:50:59 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 22 unspecified don't care: bit is 0
I0512 00:50:59.481844    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:50:59 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 12 unspecified don't care: bit is 0
I0512 00:50:59.481876    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:50:59 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 20 unspecified don't care: bit is 0
I0512 00:50:59.481910    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:50:59 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 3 bit: 13 unspecified don't care: bit is 0
I0512 00:51:00.225297    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:51:00 INFO : hyperkit: stderr: rdmsr to register 0x3a on vcpu 0
I0512 00:51:00.225348    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:51:00 INFO : hyperkit: stderr: rdmsr to register 0x140 on vcpu 0
I0512 00:51:00.244523    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:51:00 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 2 bit: 22 unspecified don't care: bit is 0
I0512 00:51:00.244581    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:51:00 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 12 unspecified don't care: bit is 0
I0512 00:51:00.244667    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:51:00 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 20 unspecified don't care: bit is 0
I0512 00:51:00.244682    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:51:00 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 3 bit: 13 unspecified don't care: bit is 0
I0512 00:51:00.245464    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:51:00 INFO : hyperkit: stderr: rdmsr to register 0x3a on vcpu 1
I0512 00:51:00.246242    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:51:00 INFO : hyperkit: stderr: rdmsr to register 0x140 on vcpu 1
I0512 00:51:09.319458    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:51:09 INFO : hyperkit: stderr: rdmsr to register 0x64d on vcpu 1
I0512 00:51:09.319876    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:51:09 INFO : hyperkit: stderr: rdmsr to register 0x64e on vcpu 1
I0512 00:51:09.319887    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:51:09 INFO : hyperkit: stderr: rdmsr to register 0x34 on vcpu 1
I0512 00:51:09.357794    1435 main.go:141] libmachine: (minikube) DBG | 2025/05/12 00:51:09 INFO : hyperkit: stderr: rdmsr to register 0xc0011029 on vcpu 1
I0512 00:51:18.402928    1435 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0512 00:51:18.402945    1435 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0512 00:51:18.403256    1435 buildroot.go:166] provisioning hostname "minikube"
I0512 00:51:18.403342    1435 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0512 00:51:18.403619    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:18.403831    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:51:18.404004    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:18.404159    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:18.404365    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:51:18.404633    1435 main.go:141] libmachine: Using SSH client type: native
I0512 00:51:18.404886    1435 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1097816a0] 0x109784380 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I0512 00:51:18.404892    1435 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0512 00:51:18.498102    1435 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0512 00:51:18.498140    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:18.498382    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:51:18.498520    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:18.498706    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:18.498891    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:51:18.499226    1435 main.go:141] libmachine: Using SSH client type: native
I0512 00:51:18.499441    1435 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1097816a0] 0x109784380 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I0512 00:51:18.499451    1435 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0512 00:51:18.572977    1435 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0512 00:51:18.573004    1435 buildroot.go:172] set auth options {CertDir:/Users/apple/.minikube CaCertPath:/Users/apple/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/apple/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/apple/.minikube/machines/server.pem ServerKeyPath:/Users/apple/.minikube/machines/server-key.pem ClientKeyPath:/Users/apple/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/apple/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/apple/.minikube}
I0512 00:51:18.573026    1435 buildroot.go:174] setting up certificates
I0512 00:51:18.573033    1435 provision.go:84] configureAuth start
I0512 00:51:18.573040    1435 main.go:141] libmachine: (minikube) Calling .GetMachineName
I0512 00:51:18.573323    1435 main.go:141] libmachine: (minikube) Calling .GetIP
I0512 00:51:18.573540    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:18.573717    1435 provision.go:143] copyHostCerts
I0512 00:51:18.573927    1435 exec_runner.go:144] found /Users/apple/.minikube/ca.pem, removing ...
I0512 00:51:18.579270    1435 exec_runner.go:203] rm: /Users/apple/.minikube/ca.pem
I0512 00:51:18.579533    1435 exec_runner.go:151] cp: /Users/apple/.minikube/certs/ca.pem --> /Users/apple/.minikube/ca.pem (1074 bytes)
I0512 00:51:18.580971    1435 exec_runner.go:144] found /Users/apple/.minikube/cert.pem, removing ...
I0512 00:51:18.580977    1435 exec_runner.go:203] rm: /Users/apple/.minikube/cert.pem
I0512 00:51:18.581068    1435 exec_runner.go:151] cp: /Users/apple/.minikube/certs/cert.pem --> /Users/apple/.minikube/cert.pem (1119 bytes)
I0512 00:51:18.582246    1435 exec_runner.go:144] found /Users/apple/.minikube/key.pem, removing ...
I0512 00:51:18.582251    1435 exec_runner.go:203] rm: /Users/apple/.minikube/key.pem
I0512 00:51:18.582423    1435 exec_runner.go:151] cp: /Users/apple/.minikube/certs/key.pem --> /Users/apple/.minikube/key.pem (1679 bytes)
I0512 00:51:18.582829    1435 provision.go:117] generating server cert: /Users/apple/.minikube/machines/server.pem ca-key=/Users/apple/.minikube/certs/ca.pem private-key=/Users/apple/.minikube/certs/ca-key.pem org=apple.minikube san=[127.0.0.1 192.168.64.2 localhost minikube]
I0512 00:51:18.719740    1435 provision.go:177] copyRemoteCerts
I0512 00:51:18.721662    1435 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0512 00:51:18.721703    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:18.722064    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:51:18.722352    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:18.722599    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:51:18.722876    1435 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I0512 00:51:18.778218    1435 ssh_runner.go:362] scp /Users/apple/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0512 00:51:18.813795    1435 ssh_runner.go:362] scp /Users/apple/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0512 00:51:18.870068    1435 ssh_runner.go:362] scp /Users/apple/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0512 00:51:18.905424    1435 provision.go:87] duration metric: took 332.355029ms to configureAuth
I0512 00:51:18.905440    1435 buildroot.go:189] setting minikube options for container-runtime
I0512 00:51:18.905686    1435 config.go:182] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0512 00:51:18.905701    1435 main.go:141] libmachine: (minikube) Calling .DriverName
I0512 00:51:18.905897    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:18.906053    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:51:18.906187    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:18.906330    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:18.906447    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:51:18.906667    1435 main.go:141] libmachine: Using SSH client type: native
I0512 00:51:18.906812    1435 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1097816a0] 0x109784380 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I0512 00:51:18.906817    1435 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0512 00:51:18.984491    1435 main.go:141] libmachine: SSH cmd err, output: <nil>: tmpfs

I0512 00:51:18.984501    1435 buildroot.go:70] root file system type: tmpfs
I0512 00:51:18.984638    1435 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0512 00:51:18.984677    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:18.984867    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:51:18.985033    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:18.985186    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:18.985335    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:51:18.985660    1435 main.go:141] libmachine: Using SSH client type: native
I0512 00:51:18.985846    1435 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1097816a0] 0x109784380 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I0512 00:51:18.985894    1435 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperkit --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0512 00:51:19.087120    1435 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperkit --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0512 00:51:19.087143    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:19.087335    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:51:19.087517    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:19.087695    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:19.087868    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:51:19.088167    1435 main.go:141] libmachine: Using SSH client type: native
I0512 00:51:19.088359    1435 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1097816a0] 0x109784380 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I0512 00:51:19.088369    1435 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0512 00:51:21.901330    1435 main.go:141] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service ‚Üí /usr/lib/systemd/system/docker.service.

I0512 00:51:21.901342    1435 machine.go:96] duration metric: took 22.59047222s to provisionDockerMachine
I0512 00:51:21.901361    1435 start.go:293] postStartSetup for "minikube" (driver="hyperkit")
I0512 00:51:21.901372    1435 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0512 00:51:21.901393    1435 main.go:141] libmachine: (minikube) Calling .DriverName
I0512 00:51:21.901734    1435 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0512 00:51:21.901749    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:21.901947    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:51:21.902107    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:21.902239    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:51:21.902452    1435 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I0512 00:51:21.973414    1435 ssh_runner.go:195] Run: cat /etc/os-release
I0512 00:51:21.980992    1435 info.go:137] Remote host: Buildroot 2023.02.9
I0512 00:51:21.981010    1435 filesync.go:126] Scanning /Users/apple/.minikube/addons for local assets ...
I0512 00:51:21.981136    1435 filesync.go:126] Scanning /Users/apple/.minikube/files for local assets ...
I0512 00:51:21.981188    1435 start.go:296] duration metric: took 79.820332ms for postStartSetup
I0512 00:51:21.981204    1435 fix.go:56] duration metric: took 23.370043571s for fixHost
I0512 00:51:21.981216    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:21.981442    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:51:21.981617    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:21.981788    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:21.981970    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:51:21.982273    1435 main.go:141] libmachine: Using SSH client type: native
I0512 00:51:21.982451    1435 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1097816a0] 0x109784380 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I0512 00:51:21.982466    1435 main.go:141] libmachine: About to run SSH command:
date +%s.%N
I0512 00:51:22.069612    1435 main.go:141] libmachine: SSH cmd err, output: <nil>: 1747036282.394742013

I0512 00:51:22.069641    1435 fix.go:216] guest clock: 1747036282.394742013
I0512 00:51:22.069649    1435 fix.go:229] Guest: 2025-05-12 00:51:22.394742013 -0700 PDT Remote: 2025-05-12 00:51:21.981206 -0700 PDT m=+23.721408616 (delta=413.536013ms)
I0512 00:51:22.069677    1435 fix.go:200] guest clock delta is within tolerance: 413.536013ms
I0512 00:51:22.069681    1435 start.go:83] releasing machines lock for "minikube", held for 23.458561958s
I0512 00:51:22.069706    1435 main.go:141] libmachine: (minikube) Calling .DriverName
I0512 00:51:22.069934    1435 main.go:141] libmachine: (minikube) Calling .GetIP
I0512 00:51:22.070092    1435 main.go:141] libmachine: (minikube) Calling .DriverName
I0512 00:51:22.070765    1435 main.go:141] libmachine: (minikube) Calling .DriverName
I0512 00:51:22.070996    1435 main.go:141] libmachine: (minikube) Calling .DriverName
I0512 00:51:22.071318    1435 ssh_runner.go:195] Run: cat /version.json
I0512 00:51:22.071333    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:22.071612    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:51:22.071887    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:22.072025    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:51:22.072175    1435 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I0512 00:51:22.073394    1435 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0512 00:51:22.073444    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:22.073786    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:51:22.073947    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:22.074146    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:51:22.074344    1435 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I0512 00:51:22.116124    1435 ssh_runner.go:195] Run: systemctl --version
I0512 00:51:24.132899    1435 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.059460831s)
W0512 00:51:24.132945    1435 start.go:867] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Operation timed out after 2005 milliseconds with 0 bytes received
I0512 00:51:24.133003    1435 ssh_runner.go:235] Completed: systemctl --version: (2.016848957s)
I0512 00:51:24.133269    1435 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0512 00:51:24.145327    1435 cni.go:209] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0512 00:51:24.145544    1435 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0512 00:51:24.172819    1435 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0512 00:51:24.172839    1435 start.go:495] detecting cgroup driver to use...
I0512 00:51:24.173368    1435 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0512 00:51:24.202005    1435 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0512 00:51:24.217810    1435 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0512 00:51:24.350127    1435 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0512 00:51:24.350297    1435 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0512 00:51:24.365952    1435 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0512 00:51:24.380169    1435 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0512 00:51:24.393482    1435 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0512 00:51:24.408016    1435 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0512 00:51:24.422691    1435 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0512 00:51:24.436989    1435 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0512 00:51:24.451466    1435 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0512 00:51:24.465797    1435 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0512 00:51:24.478623    1435 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0512 00:51:24.478729    1435 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I0512 00:51:24.493712    1435 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0512 00:51:24.513913    1435 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0512 00:51:24.704708    1435 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0512 00:51:24.735545    1435 start.go:495] detecting cgroup driver to use...
I0512 00:51:24.735691    1435 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0512 00:51:24.754183    1435 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0512 00:51:24.772951    1435 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I0512 00:51:24.798007    1435 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0512 00:51:24.818673    1435 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0512 00:51:24.837691    1435 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0512 00:51:24.893839    1435 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0512 00:51:24.915063    1435 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0512 00:51:24.941210    1435 ssh_runner.go:195] Run: which cri-dockerd
I0512 00:51:24.945957    1435 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
W0512 00:51:24.960372    1435 out.go:270] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube VM
W0512 00:51:24.960406    1435 out.go:270] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0512 00:51:24.961615    1435 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0512 00:51:25.006585    1435 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0512 00:51:25.189341    1435 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0512 00:51:25.348217    1435 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0512 00:51:25.348399    1435 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0512 00:51:25.371782    1435 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0512 00:51:25.564687    1435 ssh_runner.go:195] Run: sudo systemctl restart docker
I0512 00:51:28.313599    1435 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.748855591s)
I0512 00:51:28.313815    1435 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0512 00:51:28.349972    1435 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0512 00:51:28.375242    1435 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0512 00:51:28.399434    1435 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0512 00:51:28.591177    1435 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0512 00:51:28.784019    1435 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0512 00:51:28.958437    1435 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0512 00:51:28.985934    1435 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0512 00:51:29.005737    1435 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0512 00:51:29.186408    1435 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0512 00:51:29.276087    1435 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0512 00:51:29.281581    1435 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0512 00:51:29.288438    1435 start.go:563] Will wait 60s for crictl version
I0512 00:51:29.288549    1435 ssh_runner.go:195] Run: which crictl
I0512 00:51:29.297076    1435 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0512 00:51:29.345824    1435 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.0
RuntimeApiVersion:  v1
I0512 00:51:29.345992    1435 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0512 00:51:29.427517    1435 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0512 00:51:29.489572    1435 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.0 ...
I0512 00:51:29.492007    1435 main.go:141] libmachine: (minikube) Calling .GetIP
I0512 00:51:29.492999    1435 ssh_runner.go:195] Run: grep 192.168.64.1	host.minikube.internal$ /etc/hosts
I0512 00:51:29.503431    1435 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.64.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0512 00:51:29.526427    1435 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.64.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0512 00:51:29.526619    1435 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0512 00:51:29.526724    1435 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0512 00:51:29.560958    1435 docker.go:689] Got preloaded images: -- stdout --
nginx:latest
nginx:<none>
nginx:<none>
nginx:<none>
httpd:alpine
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
busybox:latest
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
nginx:1.18.0
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.19.8
httpd:2.4.41-alpine
busybox:1.31.0

-- /stdout --
I0512 00:51:29.560971    1435 docker.go:619] Images already preloaded, skipping extraction
I0512 00:51:29.563146    1435 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0512 00:51:29.589879    1435 docker.go:689] Got preloaded images: -- stdout --
nginx:<none>
nginx:latest
nginx:<none>
nginx:<none>
httpd:alpine
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/ingress-nginx/controller:<none>
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
busybox:latest
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
nginx:1.18.0
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.19.8
httpd:2.4.41-alpine
busybox:1.31.0

-- /stdout --
I0512 00:51:29.589901    1435 cache_images.go:84] Images are preloaded, skipping loading
I0512 00:51:29.589907    1435 kubeadm.go:934] updating node { 192.168.64.2 8443 v1.32.0 docker true true} ...
I0512 00:51:29.590003    1435 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.64.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0512 00:51:29.590078    1435 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0512 00:51:29.656400    1435 cni.go:84] Creating CNI manager for ""
I0512 00:51:29.656418    1435 cni.go:158] "hyperkit" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0512 00:51:29.656442    1435 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0512 00:51:29.656488    1435 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.64.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.64.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.64.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0512 00:51:29.656645    1435 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.64.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.64.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.64.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0512 00:51:29.656780    1435 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0512 00:51:29.676327    1435 binaries.go:44] Found k8s binaries, skipping transfer
I0512 00:51:29.676420    1435 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0512 00:51:29.689852    1435 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0512 00:51:29.716454    1435 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0512 00:51:29.741357    1435 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0512 00:51:29.767976    1435 ssh_runner.go:195] Run: grep 192.168.64.2	control-plane.minikube.internal$ /etc/hosts
I0512 00:51:29.772817    1435 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.64.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0512 00:51:29.790701    1435 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0512 00:51:29.963806    1435 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0512 00:51:29.989785    1435 certs.go:68] Setting up /Users/apple/.minikube/profiles/minikube for IP: 192.168.64.2
I0512 00:51:29.989802    1435 certs.go:194] generating shared ca certs ...
I0512 00:51:29.989820    1435 certs.go:226] acquiring lock for ca certs: {Name:mk9c7fcfc804a934001d810069a8bd2afccb033e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0512 00:51:29.992458    1435 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/apple/.minikube/ca.key
I0512 00:51:29.992698    1435 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/apple/.minikube/proxy-client-ca.key
I0512 00:51:29.992717    1435 certs.go:256] generating profile certs ...
I0512 00:51:29.993126    1435 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/apple/.minikube/profiles/minikube/client.key
I0512 00:51:29.993446    1435 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/apple/.minikube/profiles/minikube/apiserver.key.a9834ef2
I0512 00:51:29.993850    1435 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/apple/.minikube/profiles/minikube/proxy-client.key
I0512 00:51:29.994221    1435 certs.go:484] found cert: /Users/apple/.minikube/certs/ca-key.pem (1679 bytes)
I0512 00:51:29.994285    1435 certs.go:484] found cert: /Users/apple/.minikube/certs/ca.pem (1074 bytes)
I0512 00:51:29.994321    1435 certs.go:484] found cert: /Users/apple/.minikube/certs/cert.pem (1119 bytes)
I0512 00:51:29.994350    1435 certs.go:484] found cert: /Users/apple/.minikube/certs/key.pem (1679 bytes)
I0512 00:51:29.995323    1435 ssh_runner.go:362] scp /Users/apple/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0512 00:51:30.062236    1435 ssh_runner.go:362] scp /Users/apple/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0512 00:51:30.114554    1435 ssh_runner.go:362] scp /Users/apple/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0512 00:51:30.183281    1435 ssh_runner.go:362] scp /Users/apple/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0512 00:51:30.242220    1435 ssh_runner.go:362] scp /Users/apple/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0512 00:51:30.285351    1435 ssh_runner.go:362] scp /Users/apple/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0512 00:51:30.334190    1435 ssh_runner.go:362] scp /Users/apple/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0512 00:51:30.372370    1435 ssh_runner.go:362] scp /Users/apple/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0512 00:51:30.410852    1435 ssh_runner.go:362] scp /Users/apple/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0512 00:51:30.456220    1435 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0512 00:51:30.483275    1435 ssh_runner.go:195] Run: openssl version
I0512 00:51:30.490924    1435 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0512 00:51:30.508134    1435 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0512 00:51:30.514847    1435 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Mar 23 05:10 /usr/share/ca-certificates/minikubeCA.pem
I0512 00:51:30.514934    1435 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0512 00:51:30.523397    1435 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0512 00:51:30.540761    1435 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0512 00:51:30.546822    1435 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0512 00:51:30.557904    1435 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0512 00:51:30.566142    1435 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0512 00:51:30.575642    1435 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0512 00:51:30.584421    1435 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0512 00:51:30.592764    1435 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0512 00:51:30.601030    1435 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.35.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.64.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true ingress:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0512 00:51:30.601213    1435 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0512 00:51:30.626876    1435 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0512 00:51:30.643522    1435 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0512 00:51:30.643543    1435 kubeadm.go:593] restartPrimaryControlPlane start ...
I0512 00:51:30.643652    1435 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0512 00:51:30.658575    1435 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0512 00:51:30.660011    1435 kubeconfig.go:125] found "minikube" server: "https://192.168.64.2:8443"
I0512 00:51:30.666997    1435 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0512 00:51:30.680366    1435 kubeadm.go:630] The running cluster does not require reconfiguration: 192.168.64.2
I0512 00:51:30.680394    1435 kubeadm.go:1160] stopping kube-system containers ...
I0512 00:51:30.680461    1435 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0512 00:51:30.710835    1435 docker.go:483] Stopping containers: [bfb91b5eea08 315f25b92ac0 db593fbae92c b128e0508df9 5aca902c891a cd159f1b0b4c 9cdedcda3193 b9615f199fdc 416cf5a9588b 1f3948bc2566 ec842af7fd98 7f20e53f9eb0 1fdb7cbf1204 57ffc2715be5 89ffaf567f78 ab6e56120f41 470bdb94f04d bce1e712b873 d0fbc7d1555d 0eb850f2ad11 7f5871dab3fe 199e45a71476 ddc075477a20 c2b903a1076b 9aa863c5740e]
I0512 00:51:30.711021    1435 ssh_runner.go:195] Run: docker stop bfb91b5eea08 315f25b92ac0 db593fbae92c b128e0508df9 5aca902c891a cd159f1b0b4c 9cdedcda3193 b9615f199fdc 416cf5a9588b 1f3948bc2566 ec842af7fd98 7f20e53f9eb0 1fdb7cbf1204 57ffc2715be5 89ffaf567f78 ab6e56120f41 470bdb94f04d bce1e712b873 d0fbc7d1555d 0eb850f2ad11 7f5871dab3fe 199e45a71476 ddc075477a20 c2b903a1076b 9aa863c5740e
I0512 00:51:30.746667    1435 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0512 00:51:30.768706    1435 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0512 00:51:30.790229    1435 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0512 00:51:30.790241    1435 kubeadm.go:157] found existing configuration files:

I0512 00:51:30.790323    1435 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0512 00:51:30.805947    1435 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0512 00:51:30.806104    1435 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0512 00:51:30.823343    1435 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0512 00:51:30.839345    1435 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0512 00:51:30.839436    1435 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0512 00:51:30.853582    1435 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0512 00:51:30.866154    1435 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0512 00:51:30.866293    1435 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0512 00:51:30.879231    1435 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0512 00:51:30.892355    1435 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0512 00:51:30.892449    1435 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0512 00:51:30.906964    1435 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0512 00:51:30.924349    1435 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0512 00:51:31.209990    1435 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0512 00:51:33.098749    1435 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.888724331s)
I0512 00:51:33.098775    1435 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0512 00:51:33.530021    1435 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0512 00:51:33.614599    1435 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0512 00:51:33.735311    1435 api_server.go:52] waiting for apiserver process to appear ...
I0512 00:51:33.735507    1435 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0512 00:51:34.236136    1435 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0512 00:51:34.735821    1435 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0512 00:51:35.235578    1435 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0512 00:51:35.735716    1435 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0512 00:51:36.235868    1435 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0512 00:51:36.735730    1435 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0512 00:51:36.762971    1435 api_server.go:72] duration metric: took 3.027641088s to wait for apiserver process to appear ...
I0512 00:51:36.762989    1435 api_server.go:88] waiting for apiserver healthz status ...
I0512 00:51:36.763037    1435 api_server.go:253] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0512 00:51:41.763476    1435 api_server.go:269] stopped: https://192.168.64.2:8443/healthz: Get "https://192.168.64.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0512 00:51:41.763496    1435 api_server.go:253] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0512 00:51:42.233151    1435 api_server.go:279] https://192.168.64.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0512 00:51:42.233168    1435 api_server.go:103] status: https://192.168.64.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0512 00:51:42.263251    1435 api_server.go:253] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0512 00:51:42.288599    1435 api_server.go:279] https://192.168.64.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0512 00:51:42.288619    1435 api_server.go:103] status: https://192.168.64.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0512 00:51:42.764113    1435 api_server.go:253] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0512 00:51:42.772823    1435 api_server.go:279] https://192.168.64.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0512 00:51:42.772858    1435 api_server.go:103] status: https://192.168.64.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0512 00:51:43.263227    1435 api_server.go:253] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0512 00:51:43.282032    1435 api_server.go:279] https://192.168.64.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0512 00:51:43.282053    1435 api_server.go:103] status: https://192.168.64.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0512 00:51:43.763291    1435 api_server.go:253] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0512 00:51:43.784628    1435 api_server.go:279] https://192.168.64.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0512 00:51:43.784652    1435 api_server.go:103] status: https://192.168.64.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0512 00:51:44.263221    1435 api_server.go:253] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0512 00:51:44.283194    1435 api_server.go:279] https://192.168.64.2:8443/healthz returned 200:
ok
I0512 00:51:44.305993    1435 api_server.go:141] control plane version: v1.32.0
I0512 00:51:44.306019    1435 api_server.go:131] duration metric: took 7.542943939s to wait for apiserver health ...
I0512 00:51:44.306030    1435 cni.go:84] Creating CNI manager for ""
I0512 00:51:44.306045    1435 cni.go:158] "hyperkit" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0512 00:51:44.317327    1435 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0512 00:51:44.330422    1435 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0512 00:51:44.354020    1435 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0512 00:51:44.401177    1435 system_pods.go:43] waiting for kube-system pods to appear ...
I0512 00:51:44.420682    1435 system_pods.go:59] 7 kube-system pods found
I0512 00:51:44.420712    1435 system_pods.go:61] "coredns-668d6bf9bc-j5wnx" [a38cf8db-e32f-43be-83b6-ba1999ffe06d] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0512 00:51:44.420723    1435 system_pods.go:61] "etcd-minikube" [16ebd624-b82f-4f52-a5f2-156223b2c8b4] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0512 00:51:44.420728    1435 system_pods.go:61] "kube-apiserver-minikube" [e4f936d1-567f-439f-859d-0d19acf24db8] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0512 00:51:44.420732    1435 system_pods.go:61] "kube-controller-manager-minikube" [4232e720-e225-447c-8796-dae69b3150eb] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0512 00:51:44.420739    1435 system_pods.go:61] "kube-proxy-hbj6g" [5d6e75e4-ce6d-4d07-80dd-35b0075453e5] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0512 00:51:44.420745    1435 system_pods.go:61] "kube-scheduler-minikube" [5e8151b4-809e-43e2-860f-723274e040ae] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0512 00:51:44.420748    1435 system_pods.go:61] "storage-provisioner" [76a1cc6b-1aed-4b16-b0dc-c78dd08ff7da] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0512 00:51:44.420752    1435 system_pods.go:74] duration metric: took 19.566204ms to wait for pod list to return data ...
I0512 00:51:44.420759    1435 node_conditions.go:102] verifying NodePressure condition ...
I0512 00:51:44.435339    1435 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I0512 00:51:44.436529    1435 node_conditions.go:123] node cpu capacity is 2
I0512 00:51:44.436551    1435 node_conditions.go:105] duration metric: took 15.785321ms to run NodePressure ...
I0512 00:51:44.436573    1435 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0512 00:51:45.006603    1435 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0512 00:51:45.036294    1435 ops.go:34] apiserver oom_adj: -16
I0512 00:51:45.036317    1435 kubeadm.go:597] duration metric: took 14.392649131s to restartPrimaryControlPlane
I0512 00:51:45.036330    1435 kubeadm.go:394] duration metric: took 14.435191827s to StartCluster
I0512 00:51:45.036345    1435 settings.go:142] acquiring lock: {Name:mk692f39c99f57abbbaf0dc79849e928ca42cce9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0512 00:51:45.036688    1435 settings.go:150] Updating kubeconfig:  /Users/apple/.kube/config
I0512 00:51:45.038611    1435 lock.go:35] WriteFile acquiring /Users/apple/.kube/config: {Name:mk6c4066d2a31d6c55de45dd0f0224f5fa363ab0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0512 00:51:45.039210    1435 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.64.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0512 00:51:45.039355    1435 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:true ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0512 00:51:45.039437    1435 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0512 00:51:45.039444    1435 config.go:182] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0512 00:51:45.039461    1435 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0512 00:51:45.039467    1435 addons.go:247] addon storage-provisioner should already be in state true
I0512 00:51:45.039497    1435 host.go:66] Checking if "minikube" exists ...
I0512 00:51:45.039509    1435 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0512 00:51:45.039531    1435 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0512 00:51:45.039916    1435 addons.go:69] Setting ingress=true in profile "minikube"
I0512 00:51:45.039926    1435 addons.go:238] Setting addon ingress=true in "minikube"
W0512 00:51:45.039930    1435 addons.go:247] addon ingress should already be in state true
I0512 00:51:45.039930    1435 main.go:141] libmachine: Found binary path at /Users/apple/.minikube/bin/docker-machine-driver-hyperkit
I0512 00:51:45.039946    1435 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0512 00:51:45.039955    1435 host.go:66] Checking if "minikube" exists ...
I0512 00:51:45.041527    1435 main.go:141] libmachine: Found binary path at /Users/apple/.minikube/bin/docker-machine-driver-hyperkit
I0512 00:51:45.042110    1435 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0512 00:51:45.042523    1435 main.go:141] libmachine: Found binary path at /Users/apple/.minikube/bin/docker-machine-driver-hyperkit
I0512 00:51:45.042841    1435 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0512 00:51:45.052007    1435 out.go:177] üîé  Verifying Kubernetes components...
I0512 00:51:45.059882    1435 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:50238
I0512 00:51:45.060798    1435 main.go:141] libmachine: () Calling .GetVersion
I0512 00:51:45.063490    1435 main.go:141] libmachine: Using API Version  1
I0512 00:51:45.063527    1435 main.go:141] libmachine: () Calling .SetConfigRaw
I0512 00:51:45.063610    1435 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:50240
I0512 00:51:45.064373    1435 main.go:141] libmachine: () Calling .GetVersion
I0512 00:51:45.064533    1435 main.go:141] libmachine: () Calling .GetMachineName
I0512 00:51:45.065870    1435 main.go:141] libmachine: Using API Version  1
I0512 00:51:45.065882    1435 main.go:141] libmachine: () Calling .SetConfigRaw
I0512 00:51:45.065989    1435 main.go:141] libmachine: (minikube) Calling .GetState
I0512 00:51:45.070535    1435 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:50242
I0512 00:51:45.070542    1435 main.go:141] libmachine: (minikube) DBG | exe=/Users/apple/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0512 00:51:45.070555    1435 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1449
I0512 00:51:45.071095    1435 main.go:141] libmachine: () Calling .GetMachineName
I0512 00:51:45.071670    1435 main.go:141] libmachine: () Calling .GetVersion
I0512 00:51:45.072018    1435 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0512 00:51:45.072027    1435 addons.go:247] addon default-storageclass should already be in state true
I0512 00:51:45.072063    1435 host.go:66] Checking if "minikube" exists ...
I0512 00:51:45.072195    1435 main.go:141] libmachine: Found binary path at /Users/apple/.minikube/bin/docker-machine-driver-hyperkit
I0512 00:51:45.072262    1435 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0512 00:51:45.072454    1435 main.go:141] libmachine: Using API Version  1
I0512 00:51:45.072463    1435 main.go:141] libmachine: () Calling .SetConfigRaw
I0512 00:51:45.073281    1435 main.go:141] libmachine: () Calling .GetMachineName
I0512 00:51:45.073339    1435 main.go:141] libmachine: Found binary path at /Users/apple/.minikube/bin/docker-machine-driver-hyperkit
I0512 00:51:45.073477    1435 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0512 00:51:45.079852    1435 main.go:141] libmachine: Found binary path at /Users/apple/.minikube/bin/docker-machine-driver-hyperkit
I0512 00:51:45.081597    1435 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0512 00:51:45.083546    1435 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0512 00:51:45.096232    1435 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:50244
I0512 00:51:45.097542    1435 main.go:141] libmachine: () Calling .GetVersion
I0512 00:51:45.099785    1435 main.go:141] libmachine: Using API Version  1
I0512 00:51:45.099803    1435 main.go:141] libmachine: () Calling .SetConfigRaw
I0512 00:51:45.100792    1435 main.go:141] libmachine: () Calling .GetMachineName
I0512 00:51:45.101019    1435 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:50246
I0512 00:51:45.101441    1435 main.go:141] libmachine: (minikube) Calling .GetState
I0512 00:51:45.102102    1435 main.go:141] libmachine: () Calling .GetVersion
I0512 00:51:45.102238    1435 main.go:141] libmachine: (minikube) DBG | exe=/Users/apple/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0512 00:51:45.102278    1435 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1449
I0512 00:51:45.103075    1435 main.go:141] libmachine: Using API Version  1
I0512 00:51:45.103087    1435 main.go:141] libmachine: () Calling .SetConfigRaw
I0512 00:51:45.104471    1435 main.go:141] libmachine: () Calling .GetMachineName
I0512 00:51:45.104541    1435 main.go:141] libmachine: (minikube) Calling .DriverName
I0512 00:51:45.105037    1435 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:50248
I0512 00:51:45.105731    1435 main.go:141] libmachine: Found binary path at /Users/apple/.minikube/bin/docker-machine-driver-hyperkit
I0512 00:51:45.105774    1435 main.go:141] libmachine: Launching plugin server for driver hyperkit
I0512 00:51:45.105980    1435 main.go:141] libmachine: () Calling .GetVersion
I0512 00:51:45.107711    1435 main.go:141] libmachine: Using API Version  1
I0512 00:51:45.107792    1435 main.go:141] libmachine: () Calling .SetConfigRaw
I0512 00:51:45.112730    1435 main.go:141] libmachine: () Calling .GetMachineName
I0512 00:51:45.113279    1435 main.go:141] libmachine: (minikube) Calling .GetState
I0512 00:51:45.113580    1435 main.go:141] libmachine: (minikube) DBG | exe=/Users/apple/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0512 00:51:45.113727    1435 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1449
I0512 00:51:45.115562    1435 out.go:177]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/controller:v1.11.3
I0512 00:51:45.117222    1435 main.go:141] libmachine: (minikube) Calling .DriverName
I0512 00:51:45.124297    1435 main.go:141] libmachine: Plugin server listening at address 127.0.0.1:50250
I0512 00:51:45.125696    1435 main.go:141] libmachine: () Calling .GetVersion
I0512 00:51:45.129570    1435 main.go:141] libmachine: Using API Version  1
I0512 00:51:45.130489    1435 main.go:141] libmachine: () Calling .SetConfigRaw
I0512 00:51:45.131093    1435 main.go:141] libmachine: () Calling .GetMachineName
I0512 00:51:45.131496    1435 main.go:141] libmachine: (minikube) Calling .GetState
I0512 00:51:45.131716    1435 main.go:141] libmachine: (minikube) DBG | exe=/Users/apple/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0512 00:51:45.131889    1435 main.go:141] libmachine: (minikube) DBG | hyperkit pid from json: 1449
I0512 00:51:45.135130    1435 main.go:141] libmachine: (minikube) Calling .DriverName
I0512 00:51:45.135520    1435 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0512 00:51:45.135532    1435 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0512 00:51:45.135554    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:45.135738    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:51:45.135949    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:45.136195    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:51:45.136340    1435 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I0512 00:51:45.138722    1435 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0512 00:51:45.153897    1435 out.go:177]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4
I0512 00:51:45.165318    1435 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0512 00:51:45.165328    1435 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0512 00:51:45.165348    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:45.165676    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:51:45.165918    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:45.166216    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:51:45.166469    1435 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I0512 00:51:45.193354    1435 out.go:177]     ‚ñ™ Using image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4
I0512 00:51:45.203789    1435 addons.go:435] installing /etc/kubernetes/addons/ingress-deploy.yaml
I0512 00:51:45.203800    1435 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/ingress-deploy.yaml (16078 bytes)
I0512 00:51:45.203828    1435 main.go:141] libmachine: (minikube) Calling .GetSSHHostname
I0512 00:51:45.204563    1435 main.go:141] libmachine: (minikube) Calling .GetSSHPort
I0512 00:51:45.204790    1435 main.go:141] libmachine: (minikube) Calling .GetSSHKeyPath
I0512 00:51:45.205058    1435 main.go:141] libmachine: (minikube) Calling .GetSSHUsername
I0512 00:51:45.205294    1435 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/apple/.minikube/machines/minikube/id_rsa Username:docker}
I0512 00:51:45.540699    1435 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0512 00:51:45.568527    1435 api_server.go:52] waiting for apiserver process to appear ...
I0512 00:51:45.568652    1435 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0512 00:51:45.614631    1435 api_server.go:72] duration metric: took 575.393956ms to wait for apiserver process to appear ...
I0512 00:51:45.614645    1435 api_server.go:88] waiting for apiserver healthz status ...
I0512 00:51:45.614663    1435 api_server.go:253] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0512 00:51:45.663329    1435 api_server.go:279] https://192.168.64.2:8443/healthz returned 200:
ok
I0512 00:51:45.674617    1435 api_server.go:141] control plane version: v1.32.0
I0512 00:51:45.674640    1435 api_server.go:131] duration metric: took 59.985879ms to wait for apiserver health ...
I0512 00:51:45.674649    1435 system_pods.go:43] waiting for kube-system pods to appear ...
I0512 00:51:45.698094    1435 system_pods.go:59] 7 kube-system pods found
I0512 00:51:45.698130    1435 system_pods.go:61] "coredns-668d6bf9bc-j5wnx" [a38cf8db-e32f-43be-83b6-ba1999ffe06d] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0512 00:51:45.698156    1435 system_pods.go:61] "etcd-minikube" [16ebd624-b82f-4f52-a5f2-156223b2c8b4] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0512 00:51:45.698173    1435 system_pods.go:61] "kube-apiserver-minikube" [e4f936d1-567f-439f-859d-0d19acf24db8] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0512 00:51:45.698194    1435 system_pods.go:61] "kube-controller-manager-minikube" [4232e720-e225-447c-8796-dae69b3150eb] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0512 00:51:45.698203    1435 system_pods.go:61] "kube-proxy-hbj6g" [5d6e75e4-ce6d-4d07-80dd-35b0075453e5] Running
I0512 00:51:45.698213    1435 system_pods.go:61] "kube-scheduler-minikube" [5e8151b4-809e-43e2-860f-723274e040ae] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0512 00:51:45.698216    1435 system_pods.go:61] "storage-provisioner" [76a1cc6b-1aed-4b16-b0dc-c78dd08ff7da] Running
I0512 00:51:45.698222    1435 system_pods.go:74] duration metric: took 23.567411ms to wait for pod list to return data ...
I0512 00:51:45.698231    1435 kubeadm.go:582] duration metric: took 658.997498ms to wait for: map[apiserver:true system_pods:true]
I0512 00:51:45.698246    1435 node_conditions.go:102] verifying NodePressure condition ...
I0512 00:51:45.726144    1435 node_conditions.go:122] node storage ephemeral capacity is 17734596Ki
I0512 00:51:45.726234    1435 node_conditions.go:123] node cpu capacity is 2
I0512 00:51:45.726244    1435 node_conditions.go:105] duration metric: took 27.99372ms to run NodePressure ...
I0512 00:51:45.726264    1435 start.go:241] waiting for startup goroutines ...
I0512 00:51:45.766023    1435 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0512 00:51:45.852053    1435 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml
I0512 00:51:45.906732    1435 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0512 00:51:46.891799    1435 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.125731176s)
I0512 00:51:46.891843    1435 main.go:141] libmachine: Making call to close driver server
I0512 00:51:46.891856    1435 main.go:141] libmachine: (minikube) Calling .Close
I0512 00:51:46.892196    1435 main.go:141] libmachine: Successfully made call to close driver server
I0512 00:51:46.892206    1435 main.go:141] libmachine: Making call to close connection to plugin binary
I0512 00:51:46.892213    1435 main.go:141] libmachine: Making call to close driver server
I0512 00:51:46.892248    1435 main.go:141] libmachine: (minikube) Calling .Close
I0512 00:51:46.893136    1435 main.go:141] libmachine: Successfully made call to close driver server
I0512 00:51:46.893147    1435 main.go:141] libmachine: Making call to close connection to plugin binary
I0512 00:51:46.909193    1435 main.go:141] libmachine: Making call to close driver server
I0512 00:51:46.909203    1435 main.go:141] libmachine: (minikube) Calling .Close
I0512 00:51:46.909456    1435 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0512 00:51:46.909488    1435 main.go:141] libmachine: Successfully made call to close driver server
I0512 00:51:46.909494    1435 main.go:141] libmachine: Making call to close connection to plugin binary
I0512 00:51:49.515775    1435 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/ingress-deploy.yaml: (3.663599653s)
I0512 00:51:49.515862    1435 main.go:141] libmachine: Making call to close driver server
I0512 00:51:49.515881    1435 main.go:141] libmachine: (minikube) Calling .Close
I0512 00:51:49.515921    1435 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.609136997s)
I0512 00:51:49.515942    1435 main.go:141] libmachine: Making call to close driver server
I0512 00:51:49.515953    1435 main.go:141] libmachine: (minikube) Calling .Close
I0512 00:51:49.516409    1435 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0512 00:51:49.516435    1435 main.go:141] libmachine: Successfully made call to close driver server
I0512 00:51:49.516440    1435 main.go:141] libmachine: Making call to close connection to plugin binary
I0512 00:51:49.516448    1435 main.go:141] libmachine: Making call to close driver server
I0512 00:51:49.516452    1435 main.go:141] libmachine: (minikube) Calling .Close
I0512 00:51:49.516612    1435 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0512 00:51:49.516635    1435 main.go:141] libmachine: Successfully made call to close driver server
I0512 00:51:49.516639    1435 main.go:141] libmachine: Making call to close connection to plugin binary
I0512 00:51:49.516644    1435 main.go:141] libmachine: Making call to close driver server
I0512 00:51:49.516648    1435 main.go:141] libmachine: (minikube) Calling .Close
I0512 00:51:49.517680    1435 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0512 00:51:49.517727    1435 main.go:141] libmachine: Successfully made call to close driver server
I0512 00:51:49.517737    1435 main.go:141] libmachine: Making call to close connection to plugin binary
I0512 00:51:49.517775    1435 main.go:141] libmachine: Successfully made call to close driver server
I0512 00:51:49.517779    1435 main.go:141] libmachine: Making call to close connection to plugin binary
I0512 00:51:49.517789    1435 addons.go:479] Verifying addon ingress=true in "minikube"
I0512 00:51:49.517844    1435 main.go:141] libmachine: (minikube) DBG | Closing plugin on server side
I0512 00:51:49.528874    1435 out.go:177] üîé  Verifying ingress addon...
I0512 00:51:49.540435    1435 kapi.go:75] Waiting for pod with label "app.kubernetes.io/name=ingress-nginx" in ns "ingress-nginx" ...
I0512 00:51:49.551713    1435 kapi.go:86] Found 3 Pods for label selector app.kubernetes.io/name=ingress-nginx
I0512 00:51:49.551726    1435 kapi.go:107] duration metric: took 11.306758ms to wait for app.kubernetes.io/name=ingress-nginx ...
I0512 00:51:49.561925    1435 out.go:177] üåü  Enabled addons: default-storageclass, storage-provisioner, ingress
I0512 00:51:49.571917    1435 addons.go:514] duration metric: took 4.532570915s for enable addons: enabled=[default-storageclass storage-provisioner ingress]
I0512 00:51:49.571947    1435 start.go:246] waiting for cluster config update ...
I0512 00:51:49.571969    1435 start.go:255] writing updated cluster config ...
I0512 00:51:49.572698    1435 ssh_runner.go:195] Run: rm -f paused
I0512 00:51:49.805317    1435 start.go:600] kubectl: 1.33.0, cluster: 1.32.0 (minor skew: 1)
I0512 00:51:49.816265    1435 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 12 14:47:40 minikube cri-dockerd[1228]: time="2025-05-12T14:47:40Z" level=info msg="Pulling image registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0: 16797bc33772: Extracting [======================>                            ]  6.717MB/14.6MB"
May 12 14:47:42 minikube cri-dockerd[1228]: time="2025-05-12T14:47:42Z" level=info msg="Stop pulling image registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0: Status: Downloaded newer image for registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.15.0"
May 12 14:47:43 minikube dockerd[966]: time="2025-05-12T14:47:43.138956302Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 12 14:47:43 minikube dockerd[966]: time="2025-05-12T14:47:43.141916279Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 12 14:47:43 minikube dockerd[966]: time="2025-05-12T14:47:43.143301091Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 12 14:47:43 minikube dockerd[966]: time="2025-05-12T14:47:43.143853643Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 12 14:47:57 minikube cri-dockerd[1228]: time="2025-05-12T14:47:57Z" level=info msg="Pulling image quay.io/prometheus-operator/prometheus-config-reloader:v0.82.1: a8b0c7887258: Downloading [=====================================>             ]  10.11MB/13.4MB"
May 12 14:48:02 minikube cri-dockerd[1228]: time="2025-05-12T14:48:02Z" level=info msg="Stop pulling image quay.io/prometheus-operator/prometheus-config-reloader:v0.82.1: Status: Downloaded newer image for quay.io/prometheus-operator/prometheus-config-reloader:v0.82.1"
May 12 14:48:02 minikube dockerd[966]: time="2025-05-12T14:48:02.418519875Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 12 14:48:02 minikube dockerd[966]: time="2025-05-12T14:48:02.420900769Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 12 14:48:02 minikube dockerd[966]: time="2025-05-12T14:48:02.421040616Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 12 14:48:02 minikube dockerd[966]: time="2025-05-12T14:48:02.425597873Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 12 14:48:17 minikube cri-dockerd[1228]: time="2025-05-12T14:48:17Z" level=info msg="Pulling image quay.io/prometheus/alertmanager:v0.28.1: 545c14a5a415: Downloading [======================================>            ]  11.29MB/14.66MB"
May 12 14:48:27 minikube cri-dockerd[1228]: time="2025-05-12T14:48:27Z" level=info msg="Pulling image quay.io/prometheus/alertmanager:v0.28.1: 6fcbfeb5877c: Extracting [===================>                               ]  7.274MB/19.08MB"
May 12 14:48:29 minikube cri-dockerd[1228]: time="2025-05-12T14:48:29Z" level=info msg="Stop pulling image quay.io/prometheus/alertmanager:v0.28.1: Status: Downloaded newer image for quay.io/prometheus/alertmanager:v0.28.1"
May 12 14:48:29 minikube dockerd[966]: time="2025-05-12T14:48:29.721575206Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 12 14:48:29 minikube dockerd[966]: time="2025-05-12T14:48:29.721728095Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 12 14:48:29 minikube dockerd[966]: time="2025-05-12T14:48:29.721771763Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 12 14:48:29 minikube dockerd[966]: time="2025-05-12T14:48:29.721952831Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 12 14:48:43 minikube cri-dockerd[1228]: time="2025-05-12T14:48:43Z" level=info msg="Pulling image quay.io/prometheus/prometheus:v3.3.1: dd1e13a1db5e: Downloading [===>                                               ]  4.227MB/61.73MB"
May 12 14:48:53 minikube cri-dockerd[1228]: time="2025-05-12T14:48:53Z" level=info msg="Pulling image quay.io/prometheus/prometheus:v3.3.1: dd1e13a1db5e: Downloading [===========>                                       ]  13.75MB/61.73MB"
May 12 14:49:03 minikube cri-dockerd[1228]: time="2025-05-12T14:49:03Z" level=info msg="Pulling image quay.io/prometheus/prometheus:v3.3.1: 85395c032c94: Downloading [================================================>  ]  54.52MB/56.25MB"
May 12 14:49:13 minikube cri-dockerd[1228]: time="2025-05-12T14:49:13Z" level=info msg="Pulling image quay.io/prometheus/prometheus:v3.3.1: dd1e13a1db5e: Downloading [================================>                  ]   39.7MB/61.73MB"
May 12 14:49:23 minikube cri-dockerd[1228]: time="2025-05-12T14:49:23Z" level=info msg="Pulling image quay.io/prometheus/prometheus:v3.3.1: dd1e13a1db5e: Extracting [====>                                              ]  6.128MB/61.73MB"
May 12 14:49:33 minikube cri-dockerd[1228]: time="2025-05-12T14:49:33Z" level=info msg="Pulling image quay.io/prometheus/prometheus:v3.3.1: dd1e13a1db5e: Extracting [====================================>              ]  44.56MB/61.73MB"
May 12 14:49:43 minikube cri-dockerd[1228]: time="2025-05-12T14:49:43Z" level=info msg="Pulling image quay.io/prometheus/prometheus:v3.3.1: 85395c032c94: Extracting [================>                                  ]  18.38MB/56.25MB"
May 12 14:49:53 minikube cri-dockerd[1228]: time="2025-05-12T14:49:53Z" level=info msg="Pulling image quay.io/prometheus/prometheus:v3.3.1: 85395c032c94: Extracting [=============================================>     ]  51.25MB/56.25MB"
May 12 14:49:58 minikube cri-dockerd[1228]: time="2025-05-12T14:49:58Z" level=info msg="Stop pulling image quay.io/prometheus/prometheus:v3.3.1: Status: Downloaded newer image for quay.io/prometheus/prometheus:v3.3.1"
May 12 14:49:59 minikube dockerd[966]: time="2025-05-12T14:49:59.700771956Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 12 14:49:59 minikube dockerd[966]: time="2025-05-12T14:49:59.722389563Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 12 14:49:59 minikube dockerd[966]: time="2025-05-12T14:49:59.722462823Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 12 14:49:59 minikube dockerd[966]: time="2025-05-12T14:49:59.722761660Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 12 15:04:09 minikube cri-dockerd[1228]: E0512 15:04:09.714544    1228 httpstream.go:257] error forwarding port 9090 to pod a36e894bd63f9d39e793ac84e4791e0f85f3a4462424f196af17e35dd12275c4, uid : EOF:
May 12 16:09:23 minikube dockerd[966]: time="2025-05-12T16:09:23.625849185Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 12 16:09:23 minikube dockerd[966]: time="2025-05-12T16:09:23.627460571Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 12 16:09:23 minikube dockerd[966]: time="2025-05-12T16:09:23.627531122Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 12 16:09:23 minikube dockerd[966]: time="2025-05-12T16:09:23.628045367Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 12 16:09:23 minikube cri-dockerd[1228]: time="2025-05-12T16:09:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/98230a1bcdd5dfb21b2b6ab6a19138317fd16ac50b6d38d4475f1d0cc9e129c1/resolv.conf as [nameserver 10.96.0.10 search monitoring.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 12 16:09:38 minikube cri-dockerd[1228]: time="2025-05-12T16:09:38Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: 915e4f5b7bd8: Extracting [===============================================>   ]  3.342MB/3.524MB"
May 12 16:09:48 minikube cri-dockerd[1228]: time="2025-05-12T16:09:48Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: deca3a715210: Downloading [=====>                                             ]   10.8MB/107MB"
May 12 16:09:58 minikube cri-dockerd[1228]: time="2025-05-12T16:09:58Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: fe1f72baa5f5: Downloading [==============>                                    ]  17.81MB/63.47MB"
May 12 16:10:08 minikube cri-dockerd[1228]: time="2025-05-12T16:10:08Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: fe1f72baa5f5: Downloading [=================>                                 ]  22.12MB/63.47MB"
May 12 16:10:18 minikube cri-dockerd[1228]: time="2025-05-12T16:10:18Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: deca3a715210: Downloading [================>                                  ]   35.1MB/107MB"
May 12 16:10:28 minikube cri-dockerd[1228]: time="2025-05-12T16:10:28Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: deca3a715210: Downloading [===================>                               ]  41.04MB/107MB"
May 12 16:10:38 minikube cri-dockerd[1228]: time="2025-05-12T16:10:38Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: deca3a715210: Downloading [======================>                            ]  48.61MB/107MB"
May 12 16:10:48 minikube cri-dockerd[1228]: time="2025-05-12T16:10:48Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: fe1f72baa5f5: Downloading [==========================================>        ]  53.99MB/63.47MB"
May 12 16:10:58 minikube cri-dockerd[1228]: time="2025-05-12T16:10:58Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: fe1f72baa5f5: Downloading [==============================================>    ]  59.39MB/63.47MB"
May 12 16:11:08 minikube cri-dockerd[1228]: time="2025-05-12T16:11:08Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: deca3a715210: Downloading [==============================>                    ]  64.78MB/107MB"
May 12 16:11:18 minikube cri-dockerd[1228]: time="2025-05-12T16:11:18Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: deca3a715210: Downloading [================================>                  ]  69.65MB/107MB"
May 12 16:11:28 minikube cri-dockerd[1228]: time="2025-05-12T16:11:28Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: deca3a715210: Downloading [====================================>              ]  77.22MB/107MB"
May 12 16:11:38 minikube cri-dockerd[1228]: time="2025-05-12T16:11:38Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: deca3a715210: Downloading [=========================================>         ]  89.66MB/107MB"
May 12 16:11:48 minikube cri-dockerd[1228]: time="2025-05-12T16:11:48Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: deca3a715210: Downloading [===============================================>   ]    101MB/107MB"
May 12 16:11:58 minikube cri-dockerd[1228]: time="2025-05-12T16:11:58Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: deca3a715210: Extracting [==========>                                        ]  21.73MB/107MB"
May 12 16:12:08 minikube cri-dockerd[1228]: time="2025-05-12T16:12:08Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: fe1f72baa5f5: Extracting [>                                                  ]  557.1kB/63.47MB"
May 12 16:12:19 minikube cri-dockerd[1228]: time="2025-05-12T16:12:18Z" level=info msg="Pulling image docker.io/grafana/grafana:12.0.0: fe1f72baa5f5: Extracting [====================================>              ]  46.24MB/63.47MB"
May 12 16:12:23 minikube cri-dockerd[1228]: time="2025-05-12T16:12:23Z" level=info msg="Stop pulling image docker.io/grafana/grafana:12.0.0: Status: Downloaded newer image for grafana/grafana:12.0.0"
May 12 16:12:24 minikube dockerd[966]: time="2025-05-12T16:12:24.100038782Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
May 12 16:12:24 minikube dockerd[966]: time="2025-05-12T16:12:24.102008113Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
May 12 16:12:24 minikube dockerd[966]: time="2025-05-12T16:12:24.102697779Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
May 12 16:12:24 minikube dockerd[966]: time="2025-05-12T16:12:24.107365453Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1


==> container status <==
CONTAINER           IMAGE                                                                                                                            CREATED             STATE               NAME                                 ATTEMPT             POD ID              POD
b6ace020c6646       grafana/grafana@sha256:263cbefd5d9b179893c47c415daab4da5c1f3d6770154741eca4f45c81119884                                          8 minutes ago       Running             grafana                              0                   98230a1bcdd5d       grafana-db4c5c8f6-jmpl7
0abfe9aa9f332       quay.io/prometheus/prometheus@sha256:e2b8aa62b64855956e3ec1e18b4f9387fb6203174a4471936f4662f437f04405                            2 hours ago         Running             prometheus-server                    0                   a36e894bd63f9       prometheus-server-b88bf7978-pr8jw
0114ff31b3ee5       quay.io/prometheus/alertmanager@sha256:27c475db5fb156cab31d5c18a4251ac7ed567746a2483ff264516437a39b15ba                          2 hours ago         Running             alertmanager                         0                   7204d01cc0f6a       prometheus-alertmanager-0
ce2a8ea881979       quay.io/prometheus-operator/prometheus-config-reloader@sha256:22558a49909bf31d75b6a340064c553598bfd66103626868daeb107fe171f124   2 hours ago         Running             prometheus-server-configmap-reload   0                   a36e894bd63f9       prometheus-server-b88bf7978-pr8jw
52f86817b2ac1       registry.k8s.io/kube-state-metrics/kube-state-metrics@sha256:db384bf43222b066c378e77027a675d4cd9911107adba46c2922b3a55e10d6fb    2 hours ago         Running             kube-state-metrics                   0                   7556abef106b4       prometheus-kube-state-metrics-66858d7dfd-jr65v
af77a2367d928       quay.io/prometheus/pushgateway@sha256:03738d278e082ee9821df730c741b3b465c251fc2b68a85883def301a55a6215                           2 hours ago         Running             pushgateway                          0                   ff577c1e7be96       prometheus-prometheus-pushgateway-866c5c685c-j7sjt
a6394dd73bad0       quay.io/prometheus/node-exporter@sha256:d00a542e409ee618a4edc67da14dd48c5da66726bbd5537ab2af9c1dfc442c8a                         2 hours ago         Running             node-exporter                        0                   21ac90c78d635       prometheus-prometheus-node-exporter-ddfxr
946d8b2d4e5ef       nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514                                                    2 hours ago         Running             my-app-container                     0                   bcfb960d1f149       my-app-6d8f9b85c4-s72h5
a1cf06a9e91e2       nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514                                                    2 hours ago         Running             my-app-container                     0                   f1ed0e646b237       my-app-6d8f9b85c4-hntls
46e661a7910d1       nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514                                                    2 hours ago         Running             my-app-container                     0                   6456346aeeb3c       my-app-6d8f9b85c4-j97h5
77509c581ebdb       nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514                                                    2 hours ago         Running             my-app-container                     0                   fd9f8e954497b       my-app-6d8f9b85c4-frz5v
ffd2471a4ee76       nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514                                                    2 hours ago         Running             my-app-container                     0                   81d286bdaaa14       my-app-6d8f9b85c4-7f95g
1e63629e2a9fa       nginx@sha256:2bcabc23b45489fb0885d69a06ba1d648aeda973fae7bb981bafbb884165e514                                                    2 hours ago         Running             my-app-container                     0                   4e7487dfadf9c       my-app-6d8f9b85c4-sqqt4
0dc922fac2513       6e38f40d628db                                                                                                                    3 hours ago         Running             storage-provisioner                  278                 a6582e0f09e5e       storage-provisioner
3f366d2bda14d       6e38f40d628db                                                                                                                    8 hours ago         Exited              storage-provisioner                  277                 a6582e0f09e5e       storage-provisioner
f1527e2849f5e       ee44bc2368033                                                                                                                    8 hours ago         Running             controller                           3                   fb34e362a683d       ingress-nginx-controller-56d7c84fd4-gcd6m
5bdb75394b6aa       nginx@sha256:c15da6c91de8d2f436196f3a768483ad32c258ed4e1beb3d367a27ed67253e66                                                    8 hours ago         Running             nginx                                4                   91d579df3e7e0       nginx
a07ad9fcc1fac       c69fa2e9cbf5f                                                                                                                    8 hours ago         Running             coredns                              49                  baa18c6edc8eb       coredns-668d6bf9bc-j5wnx
95d457e893f1d       040f9f8aac8cd                                                                                                                    8 hours ago         Running             kube-proxy                           47                  0c41a604b5e7c       kube-proxy-hbj6g
322ae144d369f       c2e17b8d0f4a3                                                                                                                    8 hours ago         Running             kube-apiserver                       92                  281facdf6c5cc       kube-apiserver-minikube
fa4c0fae3dea9       8cab3d2a8bd0f                                                                                                                    8 hours ago         Running             kube-controller-manager              57                  b2ca56bc5d167       kube-controller-manager-minikube
538d0302ac744       a9e7e6b294baf                                                                                                                    8 hours ago         Running             etcd                                 47                  0cb34c9f68875       etcd-minikube
9250c505c92ad       a389e107f4ff1                                                                                                                    8 hours ago         Running             kube-scheduler                       47                  9e071e7f69093       kube-scheduler-minikube
075c0b96912d8       nginx@sha256:c15da6c91de8d2f436196f3a768483ad32c258ed4e1beb3d367a27ed67253e66                                                    18 hours ago        Exited              nginx                                3                   de3032bb9f2e9       nginx
228dc812402ea       ee44bc2368033                                                                                                                    18 hours ago        Exited              controller                           2                   835c80e4671b1       ingress-nginx-controller-56d7c84fd4-gcd6m
315f25b92ac02       c69fa2e9cbf5f                                                                                                                    18 hours ago        Exited              coredns                              48                  5aca902c891a9       coredns-668d6bf9bc-j5wnx
db593fbae92ce       040f9f8aac8cd                                                                                                                    18 hours ago        Exited              kube-proxy                           46                  cd159f1b0b4cb       kube-proxy-hbj6g
b9615f199fdc1       8cab3d2a8bd0f                                                                                                                    18 hours ago        Exited              kube-controller-manager              56                  ab6e56120f411       kube-controller-manager-minikube
416cf5a9588b9       c2e17b8d0f4a3                                                                                                                    18 hours ago        Exited              kube-apiserver                       91                  470bdb94f04d0       kube-apiserver-minikube
1f3948bc25668       a9e7e6b294baf                                                                                                                    18 hours ago        Exited              etcd                                 46                  ec842af7fd987       etcd-minikube
7f20e53f9eb0b       a389e107f4ff1                                                                                                                    18 hours ago        Exited              kube-scheduler                       46                  1fdb7cbf12040       kube-scheduler-minikube
3a28183945fa0       a62eeff05ba51                                                                                                                    6 days ago          Exited              patch                                1                   2958b3b15beef       ingress-nginx-admission-patch-82bcb
e700936196397       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f       6 days ago          Exited              create                               0                   6d140c8f5c520       ingress-nginx-admission-create-5sdd2


==> controller_ingress [228dc812402e] <==
I0511 22:27:01.440043       7 store.go:440] "Found valid IngressClass" ingress="default/my-ingress" ingressclass="_"
I0511 22:27:01.441125       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"my-ingress", UID:"4a8b502d-61fc-414d-8cb6-3c4a1ea9ef11", APIVersion:"networking.k8s.io/v1", ResourceVersion:"651130", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0511 22:27:01.520663       7 nginx.go:317] "Starting NGINX process"
I0511 22:27:01.522365       7 leaderelection.go:254] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0511 22:27:01.523612       7 nginx.go:337] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
W0511 22:27:01.526974       7 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0511 22:27:01.527261       7 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0511 22:27:01.528251       7 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0511 22:27:01.528400       7 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
I0511 22:27:01.529192       7 controller.go:193] "Configuration changes detected, backend reload required"
I0511 22:27:01.574235       7 leaderelection.go:268] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0511 22:27:01.574268       7 status.go:85] "New leader elected" identity="ingress-nginx-controller-56d7c84fd4-gcd6m"
I0511 22:27:01.604556       7 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-gcd6m" node="minikube"
I0511 22:27:01.630594       7 status.go:304] "updating Ingress status" namespace="default" ingress="example-ingress" currentValue=[{"ip":"192.168.64.2"}] newValue=[]
I0511 22:27:01.631857       7 status.go:304] "updating Ingress status" namespace="default" ingress="my-ingress" currentValue=[{"ip":"192.168.64.2"}] newValue=[]
I0511 22:27:01.659360       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"example-ingress", UID:"8023397e-42e4-454c-8634-a7d458a42901", APIVersion:"networking.k8s.io/v1", ResourceVersion:"653442", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0511 22:27:01.680230       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"my-ingress", UID:"4a8b502d-61fc-414d-8cb6-3c4a1ea9ef11", APIVersion:"networking.k8s.io/v1", ResourceVersion:"653443", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0511 22:27:01.857185       7 controller.go:213] "Backend successfully reloaded"
I0511 22:27:01.859022       7 controller.go:224] "Initial sync, sleeping for 1 second"
I0511 22:27:01.860959       7 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-56d7c84fd4-gcd6m", UID:"e454f57b-162e-4ce4-a9bb-3dc1f2c6c4e6", APIVersion:"v1", ResourceVersion:"653382", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
W0511 22:27:05.392453       7 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0511 22:27:05.392574       7 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0511 22:27:05.393437       7 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0511 22:27:05.393559       7 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0511 22:27:08.727627       7 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0511 22:27:08.727837       7 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0511 22:27:08.728594       7 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0511 22:27:08.729960       7 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0511 22:27:12.059668       7 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0511 22:27:12.059918       7 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0511 22:27:12.059970       7 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0511 22:27:12.060029       7 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0511 22:27:15.392547       7 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0511 22:27:15.392833       7 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0511 22:27:15.392995       7 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0511 22:27:15.393021       7 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0511 22:27:18.727443       7 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0511 22:27:18.727697       7 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0511 22:27:18.727937       7 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0511 22:27:18.728068       7 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0511 22:27:22.059888       7 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0511 22:27:22.060042       7 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0511 22:27:22.060212       7 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0511 22:27:22.060339       7 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0511 22:27:32.945631       7 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0511 22:27:32.946342       7 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0511 22:27:32.947210       7 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0511 22:27:32.947655       7 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
I0511 22:28:01.621602       7 status.go:304] "updating Ingress status" namespace="default" ingress="example-ingress" currentValue=null newValue=[{"ip":"192.168.64.2"}]
I0511 22:28:01.623169       7 status.go:304] "updating Ingress status" namespace="default" ingress="my-ingress" currentValue=null newValue=[{"ip":"192.168.64.2"}]
W0511 22:28:01.647492       7 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0511 22:28:01.648138       7 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0511 22:28:01.648925       7 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0511 22:28:01.650023       7 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
I0511 22:28:01.656131       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"example-ingress", UID:"8023397e-42e4-454c-8634-a7d458a42901", APIVersion:"networking.k8s.io/v1", ResourceVersion:"653517", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
I0511 22:28:01.658608       7 event.go:377] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"default", Name:"my-ingress", UID:"4a8b502d-61fc-414d-8cb6-3c4a1ea9ef11", APIVersion:"networking.k8s.io/v1", ResourceVersion:"653518", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
W0511 22:28:04.982733       7 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0511 22:28:04.982919       7 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0511 22:28:04.983040       7 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0511 22:28:04.983137       7 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version


==> controller_ingress [f1527e2849f5] <==
W0512 14:46:33.582379       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 14:46:33.582461       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 14:46:33.582567       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 14:46:33.582608       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 14:46:49.928720       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 14:46:49.933322       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 14:46:49.933440       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 14:46:49.933529       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 14:47:05.499491       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 14:47:05.499657       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 14:47:05.499901       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 14:47:05.499973       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 14:47:17.983504       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 14:47:17.991369       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 14:47:17.991465       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 14:47:17.991638       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 14:47:45.343628       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 14:47:45.343842       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 14:47:45.343972       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 14:47:45.344355       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 14:47:56.528835       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 14:47:56.529146       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 14:47:56.542529       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 14:47:56.543192       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 14:48:31.723129       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 14:48:31.727541       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 14:48:31.728666       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 14:48:31.729469       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 14:48:35.060023       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 14:48:35.060381       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 14:48:35.060676       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 14:48:35.060986       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 14:48:38.393897       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 14:48:38.394857       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 14:48:38.395007       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 14:48:38.395118       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 14:50:02.585134       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 14:50:02.585238       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 14:50:02.585305       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 14:50:02.585342       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 14:50:30.499541       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 14:50:30.504300       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 14:50:30.504648       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 14:50:30.505740       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 15:17:47.023446       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 15:17:47.023524       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 15:17:47.023563       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 15:17:47.027177       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 16:09:22.996113       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 16:09:22.999339       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 16:09:23.002704       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 16:09:23.004197       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 16:12:25.110921       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 16:12:25.113013       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 16:12:25.126110       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 16:12:25.126219       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version
W0512 16:12:37.205762       8 controller.go:1110] Error obtaining Endpoints for Service "default/my-service": no object matching key "default/my-service" in local store
W0512 16:12:37.208217       8 controller.go:1110] Error obtaining Endpoints for Service "default/example-service": no object matching key "default/example-service" in local store
W0512 16:12:37.208799       8 controller.go:1452] Unexpected error validating SSL certificate "default/example-tls" for server "example.local": x509: certificate relies on legacy Common Name field, use SANs instead
W0512 16:12:37.209516       8 controller.go:1453] Validating certificate against DNS names. This will be deprecated in a future version


==> coredns [315f25b92ac0] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 4b458b379caee3c38a7b1b848c22d64dfd8e6c4eb67230e9ac4c927d279a1a1d1b750c423e524590652adb4cf7862029f0f58827606ea99941e70f56d4a3e32a
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:53362 - 60954 "HINFO IN 646823959330963986.3538389436028147545. udp 56 false 512" NXDOMAIN qr,rd,ra 131 2.239288175s
[INFO] 127.0.0.1:34622 - 49872 "HINFO IN 646823959330963986.3538389436028147545. udp 56 false 512" NXDOMAIN qr,aa,rd,ra 131 0.000175442s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[508838660]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (11-May-2025 22:26:59.520) (total time: 30011ms):
Trace[508838660]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30008ms (22:27:29.528)
Trace[508838660]: [30.011428468s] [30.011428468s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1337132175]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (11-May-2025 22:26:59.530) (total time: 30004ms):
Trace[1337132175]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30004ms (22:27:29.534)
Trace[1337132175]: [30.004862698s] [30.004862698s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1821132140]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (11-May-2025 22:26:59.520) (total time: 30015ms):
Trace[1821132140]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30015ms (22:27:29.536)
Trace[1821132140]: [30.015787936s] [30.015787936s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.066926699s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.372985419s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.674324233s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.956979274s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.022206061s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.026013712s


==> coredns [a07ad9fcc1fa] <==
[INFO] 10.244.7.195:58756 - 36921 "AAAA IN prometheus-prometheus-pushgateway.monitoring.svc.monitoring.svc.cluster.local. udp 106 false 1232" NXDOMAIN qr,aa,rd 188 0.000912818s
[INFO] 10.244.7.195:51048 - 24240 "A IN prometheus-prometheus-pushgateway.monitoring.svc.monitoring.svc.cluster.local. udp 106 false 1232" NXDOMAIN qr,aa,rd 188 0.000331325s
[INFO] 10.244.7.195:60132 - 42406 "AAAA IN prometheus-prometheus-pushgateway.monitoring.svc.svc.cluster.local. udp 95 false 1232" NXDOMAIN qr,aa,rd 177 0.000260463s
[INFO] 10.244.7.195:32960 - 30568 "A IN prometheus-prometheus-pushgateway.monitoring.svc.svc.cluster.local. udp 95 false 1232" NXDOMAIN qr,aa,rd 177 0.003222501s
[INFO] 10.244.7.195:50047 - 20963 "AAAA IN prometheus-prometheus-pushgateway.monitoring.svc.cluster.local. udp 91 false 1232" NOERROR qr,aa,rd 173 0.000556407s
[INFO] 10.244.7.195:52698 - 62742 "A IN prometheus-prometheus-pushgateway.monitoring.svc.cluster.local. udp 91 false 1232" NOERROR qr,aa,rd 158 0.000356812s
[INFO] 10.244.7.195:52918 - 31114 "AAAA IN kubernetes.default.svc.monitoring.svc.cluster.local. udp 80 false 1232" NXDOMAIN qr,aa,rd 162 0.000318339s
[INFO] 10.244.7.195:44156 - 36937 "A IN kubernetes.default.svc.monitoring.svc.cluster.local. udp 80 false 1232" NXDOMAIN qr,aa,rd 162 0.014617423s
[INFO] 10.244.7.195:56350 - 8445 "A IN kubernetes.default.svc.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.008436342s
[INFO] 10.244.7.195:54165 - 37835 "AAAA IN kubernetes.default.svc.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.001014288s
[INFO] 10.244.7.195:37557 - 62754 "A IN kubernetes.default.svc.cluster.local. udp 65 false 1232" NOERROR qr,aa,rd 106 0.00081119s
[INFO] 10.244.7.195:43100 - 38515 "AAAA IN kubernetes.default.svc.cluster.local. udp 65 false 1232" NOERROR qr,aa,rd 147 0.000631681s
[INFO] 10.244.7.195:59958 - 36563 "AAAA IN kubernetes.default.svc.monitoring.svc.cluster.local. udp 80 false 1232" NXDOMAIN qr,aa,rd 162 0.000272563s
[INFO] 10.244.7.195:49725 - 48155 "A IN kubernetes.default.svc.monitoring.svc.cluster.local. udp 80 false 1232" NXDOMAIN qr,aa,rd 162 0.001222786s
[INFO] 10.244.7.195:33783 - 65442 "A IN kubernetes.default.svc.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000277961s
[INFO] 10.244.7.195:33895 - 24709 "AAAA IN kubernetes.default.svc.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000305146s
[INFO] 10.244.7.195:44462 - 6054 "A IN kubernetes.default.svc.cluster.local. udp 65 false 1232" NOERROR qr,aa,rd 106 0.000404297s
[INFO] 10.244.7.195:54506 - 4139 "AAAA IN kubernetes.default.svc.cluster.local. udp 65 false 1232" NOERROR qr,aa,rd 147 0.001645573s
[INFO] 10.244.7.195:47301 - 44928 "A IN prometheus-prometheus-pushgateway.monitoring.svc.monitoring.svc.cluster.local. udp 106 false 1232" NXDOMAIN qr,aa,rd 188 0.000199304s
[INFO] 10.244.7.195:35764 - 44762 "AAAA IN prometheus-prometheus-pushgateway.monitoring.svc.monitoring.svc.cluster.local. udp 106 false 1232" NXDOMAIN qr,aa,rd 188 0.001786941s
[INFO] 10.244.7.195:46956 - 8277 "A IN prometheus-prometheus-pushgateway.monitoring.svc.svc.cluster.local. udp 95 false 1232" NXDOMAIN qr,aa,rd 177 0.000245781s
[INFO] 10.244.7.195:60754 - 4203 "AAAA IN prometheus-prometheus-pushgateway.monitoring.svc.svc.cluster.local. udp 95 false 1232" NXDOMAIN qr,aa,rd 177 0.000683571s
[INFO] 10.244.7.195:55714 - 22121 "A IN prometheus-prometheus-pushgateway.monitoring.svc.cluster.local. udp 91 false 1232" NOERROR qr,aa,rd 158 0.000255277s
[INFO] 10.244.7.195:50899 - 60292 "AAAA IN prometheus-prometheus-pushgateway.monitoring.svc.cluster.local. udp 91 false 1232" NOERROR qr,aa,rd 173 0.002601148s
[INFO] 10.244.7.195:57996 - 27527 "A IN kubernetes.default.svc.monitoring.svc.cluster.local. udp 80 false 1232" NXDOMAIN qr,aa,rd 162 0.000305653s
[INFO] 10.244.7.195:54361 - 9736 "AAAA IN kubernetes.default.svc.monitoring.svc.cluster.local. udp 80 false 1232" NXDOMAIN qr,aa,rd 162 0.000930452s
[INFO] 10.244.7.195:35553 - 43210 "A IN kubernetes.default.svc.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000402506s
[INFO] 10.244.7.195:42515 - 4000 "AAAA IN kubernetes.default.svc.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.00064239s
[INFO] 10.244.7.195:56613 - 5409 "A IN kubernetes.default.svc.cluster.local. udp 65 false 1232" NOERROR qr,aa,rd 106 0.000552934s
[INFO] 10.244.7.195:41020 - 24649 "AAAA IN kubernetes.default.svc.cluster.local. udp 65 false 1232" NOERROR qr,aa,rd 147 0.000318547s
[INFO] 10.244.7.195:49575 - 6268 "A IN kubernetes.default.svc.monitoring.svc.cluster.local. udp 80 false 1232" NXDOMAIN qr,aa,rd 162 0.000182912s
[INFO] 10.244.7.195:48645 - 43374 "AAAA IN kubernetes.default.svc.monitoring.svc.cluster.local. udp 80 false 1232" NXDOMAIN qr,aa,rd 162 0.0010385s
[INFO] 10.244.7.195:59092 - 5089 "A IN kubernetes.default.svc.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000258275s
[INFO] 10.244.7.195:52520 - 23801 "AAAA IN kubernetes.default.svc.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.001082479s
[INFO] 10.244.7.195:55293 - 49193 "A IN kubernetes.default.svc.cluster.local. udp 65 false 1232" NOERROR qr,aa,rd 106 0.000211298s
[INFO] 10.244.7.195:58017 - 25947 "AAAA IN kubernetes.default.svc.cluster.local. udp 65 false 1232" NOERROR qr,aa,rd 147 0.000876578s
[INFO] 10.244.7.197:35102 - 27080 "AAAA IN grafana.com.monitoring.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.001492325s
[INFO] 10.244.7.197:55557 - 33995 "A IN grafana.com.monitoring.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000305057s
[INFO] 10.244.7.197:33064 - 48170 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000345312s
[INFO] 10.244.7.197:34018 - 47093 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000236634s
[INFO] 10.244.7.197:42103 - 29381 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000283083s
[INFO] 10.244.7.197:57564 - 1604 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000227145s
[INFO] 10.244.7.197:55095 - 25172 "A IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 56 1.151122198s
[INFO] 10.244.7.197:40714 - 43146 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,rd,ra 68 1.151038942s
[INFO] 10.244.7.197:42640 - 59406 "AAAA IN grafana.com.monitoring.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000255823s
[INFO] 10.244.7.197:36624 - 2514 "A IN grafana.com.monitoring.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.001092116s
[INFO] 10.244.7.197:47956 - 10400 "AAAA IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000291185s
[INFO] 10.244.7.197:44209 - 8580 "A IN grafana.com.svc.cluster.local. udp 58 false 1232" NXDOMAIN qr,aa,rd 140 0.000204878s
[INFO] 10.244.7.197:38488 - 17775 "A IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000289486s
[INFO] 10.244.7.197:52531 - 55802 "AAAA IN grafana.com.cluster.local. udp 54 false 1232" NXDOMAIN qr,aa,rd 136 0.000143844s
[INFO] 10.244.7.197:54557 - 59709 "A IN grafana.com. udp 40 false 1232" NOERROR qr,aa,rd,ra 56 0.000088305s
[INFO] 10.244.7.197:49892 - 1182 "AAAA IN grafana.com. udp 40 false 1232" NOERROR qr,aa,rd,ra 68 0.001948588s
[INFO] 10.244.7.197:33783 - 30071 "A IN storage.googleapis.com.monitoring.svc.cluster.local. udp 80 false 1232" NXDOMAIN qr,aa,rd 162 0.001447357s
[INFO] 10.244.7.197:57824 - 65171 "AAAA IN storage.googleapis.com.monitoring.svc.cluster.local. udp 80 false 1232" NXDOMAIN qr,aa,rd 162 0.003397179s
[INFO] 10.244.7.197:48000 - 15094 "A IN storage.googleapis.com.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000289891s
[INFO] 10.244.7.197:40609 - 13051 "AAAA IN storage.googleapis.com.svc.cluster.local. udp 69 false 1232" NXDOMAIN qr,aa,rd 151 0.000371803s
[INFO] 10.244.7.197:57640 - 20976 "A IN storage.googleapis.com.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000174514s
[INFO] 10.244.7.197:56214 - 1581 "AAAA IN storage.googleapis.com.cluster.local. udp 65 false 1232" NXDOMAIN qr,aa,rd 147 0.000135356s
[INFO] 10.244.7.197:44929 - 19237 "A IN storage.googleapis.com. udp 51 false 1232" NOERROR qr,rd,ra 344 0.056753606s
[INFO] 10.244.7.197:51123 - 22944 "AAAA IN storage.googleapis.com. udp 51 false 1232" NOERROR qr,rd,ra 240 0.057965515s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_03_22T22_10_54_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 23 Mar 2025 05:10:51 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 12 May 2025 16:20:55 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 12 May 2025 16:16:53 +0000   Sat, 03 May 2025 19:40:52 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 12 May 2025 16:16:53 +0000   Sat, 03 May 2025 19:40:52 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 12 May 2025 16:16:53 +0000   Sat, 03 May 2025 19:40:52 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 12 May 2025 16:16:53 +0000   Mon, 12 May 2025 07:51:49 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.64.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             3912944Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17734596Ki
  hugepages-2Mi:      0
  memory:             3912944Ki
  pods:               110
System Info:
  Machine ID:                 437e79e19fcc490db261eb2a21bcf789
  System UUID:                ed404ca4-0000-0000-9fcb-89a08dc27b56
  Boot ID:                    2bd1910b-38fd-4649-b604-6912ccb1717b
  Kernel Version:             5.10.207
  OS Image:                   Buildroot 2023.02.9
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.0
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (21 in total)
  Namespace                   Name                                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                  ------------  ----------  ---------------  -------------  ---
  backend                     nginx                                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         9d
  default                     my-app-6d8f9b85c4-7f95g                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         124m
  default                     my-app-6d8f9b85c4-frz5v                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         124m
  default                     my-app-6d8f9b85c4-hntls                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m
  default                     my-app-6d8f9b85c4-j97h5                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m
  default                     my-app-6d8f9b85c4-s72h5                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         122m
  default                     my-app-6d8f9b85c4-sqqt4                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         124m
  ingress-nginx               ingress-nginx-controller-56d7c84fd4-gcd6m             100m (5%)     0 (0%)      90Mi (2%)        0 (0%)         6d2h
  kube-system                 coredns-668d6bf9bc-j5wnx                              100m (5%)     0 (0%)      70Mi (1%)        170Mi (4%)     50d
  kube-system                 etcd-minikube                                         100m (5%)     0 (0%)      100Mi (2%)       0 (0%)         50d
  kube-system                 kube-apiserver-minikube                               250m (12%)    0 (0%)      0 (0%)           0 (0%)         50d
  kube-system                 kube-controller-manager-minikube                      200m (10%)    0 (0%)      0 (0%)           0 (0%)         50d
  kube-system                 kube-proxy-hbj6g                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         50d
  kube-system                 kube-scheduler-minikube                               100m (5%)     0 (0%)      0 (0%)           0 (0%)         50d
  kube-system                 storage-provisioner                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         50d
  monitoring                  grafana-db4c5c8f6-jmpl7                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         11m
  monitoring                  prometheus-alertmanager-0                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         94m
  monitoring                  prometheus-kube-state-metrics-66858d7dfd-jr65v        0 (0%)        0 (0%)      0 (0%)           0 (0%)         94m
  monitoring                  prometheus-prometheus-node-exporter-ddfxr             0 (0%)        0 (0%)      0 (0%)           0 (0%)         94m
  monitoring                  prometheus-prometheus-pushgateway-866c5c685c-j7sjt    0 (0%)        0 (0%)      0 (0%)           0 (0%)         94m
  monitoring                  prometheus-server-b88bf7978-pr8jw                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         94m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (42%)  0 (0%)
  memory             260Mi (6%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[May12 11:02] ERROR: earlyprintk= earlyser already used
[  +0.000000] You have booted with nomodeset. This means your GPU drivers are DISABLED
[  +0.000001] Any video related functionality will be severely degraded, and you may not even be able to suspend the system properly
[  +0.000000] Unless you actually understand what nomodeset does, you should reboot without enabling it
[  +0.191680] ACPI BIOS Warning (bug): Incorrect checksum in table [DSDT] - 0xBE, should be 0x1B (20200925/tbprint-173)
[  +0.004528] RETBleed: WARNING: Spectre v2 mitigation leaves CPU vulnerable to RETBleed attacks, data leaks possible!
[  +9.764746] ACPI Error: Could not enable RealTimeClock event (20200925/evxfevnt-182)
[  +0.000003] ACPI Warning: Could not enable fixed event - RealTimeClock (4) (20200925/evxface-618)
[  +0.010184] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[  +4.139672] systemd-fstab-generator[126]: Ignoring "noauto" option for root device
[  +2.346778] NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory
[  +0.000005] NFSD: unable to find recovery directory /var/lib/nfs/v4recovery
[  +0.000001] NFSD: Unable to initialize client recovery tracking! (-2)
[  +3.391957] systemd-fstab-generator[476]: Ignoring "noauto" option for root device
[  +0.188495] systemd-fstab-generator[494]: Ignoring "noauto" option for root device
[  +5.147329] systemd-fstab-generator[887]: Ignoring "noauto" option for root device
[  +0.119602] kauditd_printk_skb: 75 callbacks suppressed
[  +0.376981] systemd-fstab-generator[925]: Ignoring "noauto" option for root device
[  +0.183360] systemd-fstab-generator[937]: Ignoring "noauto" option for root device
[  +0.177593] systemd-fstab-generator[951]: Ignoring "noauto" option for root device
[  +3.058438] systemd-fstab-generator[1180]: Ignoring "noauto" option for root device
[  +0.201384] systemd-fstab-generator[1192]: Ignoring "noauto" option for root device
[  +0.166373] systemd-fstab-generator[1204]: Ignoring "noauto" option for root device
[  +0.216106] systemd-fstab-generator[1220]: Ignoring "noauto" option for root device
[  +0.784077] systemd-fstab-generator[1349]: Ignoring "noauto" option for root device
[  +0.101079] kauditd_printk_skb: 206 callbacks suppressed
[May12 11:03] systemd-fstab-generator[1469]: Ignoring "noauto" option for root device
[  +2.452312] kauditd_printk_skb: 74 callbacks suppressed
[  +9.435332] systemd-fstab-generator[2093]: Ignoring "noauto" option for root device
[  +7.479899] kauditd_printk_skb: 56 callbacks suppressed
[  +5.584637] kauditd_printk_skb: 27 callbacks suppressed
[ +13.422261] kauditd_printk_skb: 51 callbacks suppressed
[ +12.780063] clocksource: timekeeping watchdog on CPU1: Marking clocksource 'tsc' as unstable because the skew is too large:
[  +0.000144] clocksource:                       'hpet' wd_now: 324b8497 wd_last: 317f761a mask: ffffffff
[  +0.000080] clocksource:                       'tsc' cs_now: 96c9437f0ba cs_last: 96bdcc78c48 mask: ffffffffffffffff
[  +0.000281] TSC found unstable after boot, most likely due to broken BIOS. Use 'tsc=unstable'.
[  +0.012757] clocksource: Checking clocksource tsc synchronization from CPU 1.
[May12 11:04] hrtimer: interrupt took 8486299 ns
[May12 11:59] kauditd_printk_skb: 26 callbacks suppressed
[  +6.196045] kauditd_printk_skb: 4 callbacks suppressed
[May12 12:16] kauditd_printk_skb: 2 callbacks suppressed
[May12 12:23] kauditd_printk_skb: 22 callbacks suppressed
[May12 14:29] kauditd_printk_skb: 31 callbacks suppressed
[May12 14:39] kauditd_printk_skb: 50 callbacks suppressed
[May12 14:40] kauditd_printk_skb: 26 callbacks suppressed
[  +5.024525] kauditd_printk_skb: 33 callbacks suppressed
[  +5.014895] kauditd_printk_skb: 41 callbacks suppressed
[May12 15:08] kauditd_printk_skb: 8 callbacks suppressed
[ +19.297283] kauditd_printk_skb: 33 callbacks suppressed
[May12 16:12] kauditd_printk_skb: 6 callbacks suppressed


==> etcd [1f3948bc2566] <==
{"level":"info","ts":"2025-05-11T22:36:31.436252Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1549593362,"revision":653741,"compact-revision":652925}
{"level":"warn","ts":"2025-05-11T22:41:12.026592Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"331.107939ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-11T22:41:12.026695Z","caller":"traceutil/trace.go:171","msg":"trace[332948029] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:654292; }","duration":"331.268107ms","start":"2025-05-11T22:41:11.695400Z","end":"2025-05-11T22:41:12.026670Z","steps":["trace[332948029] 'range keys from in-memory index tree'  (duration: 330.788692ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-11T22:41:12.026754Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-11T22:41:11.695366Z","time spent":"331.3591ms","remote":"127.0.0.1:36712","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-05-11T22:41:30.774384Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":654032}
{"level":"info","ts":"2025-05-11T22:41:30.780118Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":654032,"took":"4.967135ms","hash":1049620355,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":1912832,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-05-11T22:41:30.780285Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1049620355,"revision":654032,"compact-revision":653741}
{"level":"info","ts":"2025-05-11T22:41:31.194545Z","caller":"traceutil/trace.go:171","msg":"trace[1877679635] transaction","detail":"{read_only:false; response_revision:654311; number_of_response:1; }","duration":"162.024359ms","start":"2025-05-11T22:41:31.032475Z","end":"2025-05-11T22:41:31.194500Z","steps":["trace[1877679635] 'process raft request'  (duration: 161.278303ms)"],"step_count":1}
{"level":"info","ts":"2025-05-11T22:43:07.396920Z","caller":"traceutil/trace.go:171","msg":"trace[1070830063] linearizableReadLoop","detail":"{readStateIndex:781079; appliedIndex:781079; }","duration":"219.91834ms","start":"2025-05-11T22:43:07.176968Z","end":"2025-05-11T22:43:07.396885Z","steps":["trace[1070830063] 'read index received'  (duration: 219.868915ms)","trace[1070830063] 'applied index is now lower than readState.Index'  (duration: 22.712¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-05-11T22:43:07.397331Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"220.385974ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-11T22:43:07.397469Z","caller":"traceutil/trace.go:171","msg":"trace[1235343974] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:654399; }","duration":"220.562062ms","start":"2025-05-11T22:43:07.176878Z","end":"2025-05-11T22:43:07.397439Z","steps":["trace[1235343974] 'agreement among raft nodes before linearized reading'  (duration: 220.308435ms)"],"step_count":1}
{"level":"info","ts":"2025-05-11T22:43:07.420535Z","caller":"traceutil/trace.go:171","msg":"trace[581008841] transaction","detail":"{read_only:false; response_revision:654400; number_of_response:1; }","duration":"233.324242ms","start":"2025-05-11T22:43:07.187177Z","end":"2025-05-11T22:43:07.420500Z","steps":["trace[581008841] 'process raft request'  (duration: 232.230995ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-11T22:43:43.295604Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"132.317826ms","expected-duration":"100ms","prefix":"","request":"header:<ID:12750419252673149412 > lease_revoke:<id:30f296c174e279c8>","response":"size:29"}
{"level":"info","ts":"2025-05-11T22:43:43.322274Z","caller":"traceutil/trace.go:171","msg":"trace[1754046553] linearizableReadLoop","detail":"{readStateIndex:781088; appliedIndex:781087; }","duration":"195.749426ms","start":"2025-05-11T22:43:43.110365Z","end":"2025-05-11T22:43:43.306115Z","steps":["trace[1754046553] 'read index received'  (duration: 36.567875ms)","trace[1754046553] 'applied index is now lower than readState.Index'  (duration: 159.152837ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-11T22:43:43.347572Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"218.621656ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-11T22:43:43.351822Z","caller":"traceutil/trace.go:171","msg":"trace[567992063] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:654407; }","duration":"241.475177ms","start":"2025-05-11T22:43:43.110264Z","end":"2025-05-11T22:43:43.351740Z","steps":["trace[567992063] 'agreement among raft nodes before linearized reading'  (duration: 210.269482ms)"],"step_count":1}
{"level":"info","ts":"2025-05-11T22:44:33.742136Z","caller":"traceutil/trace.go:171","msg":"trace[1275188806] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:654462; }","duration":"151.885105ms","start":"2025-05-11T22:44:33.589862Z","end":"2025-05-11T22:44:33.741746Z","steps":[],"step_count":0}
{"level":"info","ts":"2025-05-11T22:44:34.109779Z","caller":"traceutil/trace.go:171","msg":"trace[713726385] linearizableReadLoop","detail":"{readStateIndex:781154; appliedIndex:781154; }","duration":"143.249589ms","start":"2025-05-11T22:44:33.966492Z","end":"2025-05-11T22:44:34.109742Z","steps":["trace[713726385] 'read index received'  (duration: 143.199964ms)","trace[713726385] 'applied index is now lower than readState.Index'  (duration: 22.411¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-05-11T22:44:34.110928Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"144.305017ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ranges/serviceips\" limit:1 ","response":"range_response_count:1 size:111239"}
{"level":"info","ts":"2025-05-11T22:44:34.111049Z","caller":"traceutil/trace.go:171","msg":"trace[2098284027] range","detail":"{range_begin:/registry/ranges/serviceips; range_end:; response_count:1; response_revision:654462; }","duration":"144.558143ms","start":"2025-05-11T22:44:33.966431Z","end":"2025-05-11T22:44:34.110988Z","steps":["trace[2098284027] 'agreement among raft nodes before linearized reading'  (duration: 143.471ms)"],"step_count":1}
{"level":"info","ts":"2025-05-11T22:44:35.142996Z","caller":"traceutil/trace.go:171","msg":"trace[1689681062] transaction","detail":"{read_only:false; response_revision:654464; number_of_response:1; }","duration":"149.59556ms","start":"2025-05-11T22:44:34.993348Z","end":"2025-05-11T22:44:35.142961Z","steps":["trace[1689681062] 'process raft request'  (duration: 149.336831ms)"],"step_count":1}
{"level":"info","ts":"2025-05-11T22:44:57.229323Z","caller":"traceutil/trace.go:171","msg":"trace[1790717873] linearizableReadLoop","detail":"{readStateIndex:781180; appliedIndex:781180; }","duration":"109.716631ms","start":"2025-05-11T22:44:57.119573Z","end":"2025-05-11T22:44:57.229290Z","steps":["trace[1790717873] 'read index received'  (duration: 109.669507ms)","trace[1790717873] 'applied index is now lower than readState.Index'  (duration: 19.41¬µs)"],"step_count":2}
{"level":"warn","ts":"2025-05-11T22:44:57.230135Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"110.048597ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/default/webapp\" limit:1 ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-11T22:44:57.230277Z","caller":"traceutil/trace.go:171","msg":"trace[1717283185] range","detail":"{range_begin:/registry/deployments/default/webapp; range_end:; response_count:0; response_revision:654484; }","duration":"110.749247ms","start":"2025-05-11T22:44:57.119498Z","end":"2025-05-11T22:44:57.230248Z","steps":["trace[1717283185] 'agreement among raft nodes before linearized reading'  (duration: 109.957351ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-11T22:44:57.799928Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"409.198097ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-11T22:44:57.800878Z","caller":"traceutil/trace.go:171","msg":"trace[478112020] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:654484; }","duration":"410.177987ms","start":"2025-05-11T22:44:57.390667Z","end":"2025-05-11T22:44:57.800846Z","steps":["trace[478112020] 'range keys from in-memory index tree'  (duration: 405.057328ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-11T22:44:57.802052Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"412.469631ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/default/web-app\" limit:1 ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-11T22:44:57.802365Z","caller":"traceutil/trace.go:171","msg":"trace[646280690] range","detail":"{range_begin:/registry/deployments/default/web-app; range_end:; response_count:0; response_revision:654484; }","duration":"413.133463ms","start":"2025-05-11T22:44:57.389205Z","end":"2025-05-11T22:44:57.802340Z","steps":["trace[646280690] 'range keys from in-memory index tree'  (duration: 163.139729ms)","trace[646280690] 'filter and sort the key-value pairs'  (duration: 244.50209ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-11T22:44:57.833411Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-11T22:44:57.389087Z","time spent":"444.18078ms","remote":"127.0.0.1:37168","response type":"/etcdserverpb.KV/Range","request count":0,"request size":41,"response count":0,"response size":29,"request content":"key:\"/registry/deployments/default/web-app\" limit:1 "}
{"level":"info","ts":"2025-05-11T22:45:03.670720Z","caller":"traceutil/trace.go:171","msg":"trace[538144125] transaction","detail":"{read_only:false; response_revision:654489; number_of_response:1; }","duration":"121.073307ms","start":"2025-05-11T22:45:03.549549Z","end":"2025-05-11T22:45:03.670599Z","steps":["trace[538144125] 'process raft request'  (duration: 120.357249ms)"],"step_count":1}
{"level":"info","ts":"2025-05-12T03:56:51.424190Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":654310}
{"level":"info","ts":"2025-05-12T03:56:51.446262Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":654310,"took":"13.710744ms","hash":615984814,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":1855488,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-05-12T03:56:51.456071Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":615984814,"revision":654310,"compact-revision":654032}
{"level":"warn","ts":"2025-05-12T03:58:44.453410Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.138725ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-12T03:58:44.454167Z","caller":"traceutil/trace.go:171","msg":"trace[1527122971] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:654676; }","duration":"111.928094ms","start":"2025-05-12T03:58:44.342200Z","end":"2025-05-12T03:58:44.454128Z","steps":["trace[1527122971] 'range keys from in-memory index tree'  (duration: 109.758852ms)"],"step_count":1}
{"level":"info","ts":"2025-05-12T04:01:51.440817Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":654570}
{"level":"info","ts":"2025-05-12T04:01:51.447130Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":654570,"took":"4.315363ms","hash":2592263314,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":1896448,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-05-12T04:01:51.447892Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2592263314,"revision":654570,"compact-revision":654310}
{"level":"info","ts":"2025-05-12T04:06:51.455194Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":654849}
{"level":"info","ts":"2025-05-12T04:06:51.459561Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":654849,"took":"3.206538ms","hash":4140338742,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":1945600,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-05-12T04:06:51.460459Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4140338742,"revision":654849,"compact-revision":654570}
{"level":"info","ts":"2025-05-12T04:11:51.475281Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":655128}
{"level":"info","ts":"2025-05-12T04:11:51.479466Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":655128,"took":"3.366362ms","hash":3603974368,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":1933312,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-05-12T04:11:51.479640Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3603974368,"revision":655128,"compact-revision":654849}
{"level":"info","ts":"2025-05-12T04:16:51.494087Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":655409}
{"level":"info","ts":"2025-05-12T04:16:51.498882Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":655409,"took":"4.424931ms","hash":3769579294,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":1933312,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-05-12T04:16:51.498964Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3769579294,"revision":655409,"compact-revision":655128}
{"level":"info","ts":"2025-05-12T04:21:51.511979Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":655688}
{"level":"info","ts":"2025-05-12T04:21:51.516069Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":655688,"took":"3.051387ms","hash":1574333742,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":1933312,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-05-12T04:21:51.516907Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1574333742,"revision":655688,"compact-revision":655409}
{"level":"info","ts":"2025-05-12T04:26:51.530595Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":655968}
{"level":"info","ts":"2025-05-12T04:26:51.535511Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":655968,"took":"3.607694ms","hash":1836224752,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":1945600,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-05-12T04:26:51.535683Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1836224752,"revision":655968,"compact-revision":655688}
{"level":"info","ts":"2025-05-12T04:31:51.547779Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":656248}
{"level":"info","ts":"2025-05-12T04:31:51.553598Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":656248,"took":"4.136454ms","hash":2586243891,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":1941504,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-05-12T04:31:51.553831Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2586243891,"revision":656248,"compact-revision":655968}
{"level":"info","ts":"2025-05-12T04:36:51.564021Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":656526}
{"level":"info","ts":"2025-05-12T04:36:51.568853Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":656526,"took":"3.310098ms","hash":1678771384,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":1945600,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-05-12T04:36:51.568945Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1678771384,"revision":656526,"compact-revision":656248}
{"level":"info","ts":"2025-05-12T04:38:43.696690Z","caller":"traceutil/trace.go:171","msg":"trace[670825015] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:656924; }","duration":"6.095395264s","start":"2025-05-12T04:38:37.600313Z","end":"2025-05-12T04:38:43.696653Z","steps":["trace[670825015] 'agreement among raft nodes before linearized reading'  (duration: 6.095272113s)"],"step_count":1}


==> etcd [538d0302ac74] <==
{"level":"warn","ts":"2025-05-12T14:49:42.551409Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"289.662117ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-12T14:49:42.578440Z","caller":"traceutil/trace.go:171","msg":"trace[5597062] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:671607; }","duration":"316.772578ms","start":"2025-05-12T14:49:42.261618Z","end":"2025-05-12T14:49:42.578365Z","steps":["trace[5597062] 'agreement among raft nodes before linearized reading'  (duration: 288.472074ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-12T14:49:42.562460Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"320.678694ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"warn","ts":"2025-05-12T14:49:42.578100Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"136.564865ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2025-05-12T14:49:42.580219Z","caller":"traceutil/trace.go:171","msg":"trace[748435502] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:671607; }","duration":"138.879892ms","start":"2025-05-12T14:49:42.441284Z","end":"2025-05-12T14:49:42.580162Z","steps":["trace[748435502] 'agreement among raft nodes before linearized reading'  (duration: 100.650701ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-12T14:49:42.580733Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-12T14:49:42.261563Z","time spent":"319.116387ms","remote":"127.0.0.1:51374","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-05-12T14:49:42.582791Z","caller":"traceutil/trace.go:171","msg":"trace[2021979734] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:671607; }","duration":"341.08302ms","start":"2025-05-12T14:49:42.241661Z","end":"2025-05-12T14:49:42.582746Z","steps":["trace[2021979734] 'agreement among raft nodes before linearized reading'  (duration: 316.36094ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-12T14:49:42.582899Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-12T14:49:42.241469Z","time spent":"341.379332ms","remote":"127.0.0.1:51378","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-05-12T14:50:07.392890Z","caller":"traceutil/trace.go:171","msg":"trace[1259369462] transaction","detail":"{read_only:false; response_revision:671636; number_of_response:1; }","duration":"316.198681ms","start":"2025-05-12T14:50:07.076636Z","end":"2025-05-12T14:50:07.392833Z","steps":["trace[1259369462] 'process raft request'  (duration: 287.542491ms)","trace[1259369462] 'compare'  (duration: 27.947364ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-12T14:50:07.393119Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-12T14:50:07.076469Z","time spent":"316.404473ms","remote":"127.0.0.1:51412","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":117,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.64.2\" mod_revision:671620 > success:<request_put:<key:\"/registry/masterleases/192.168.64.2\" value_size:66 lease:3527047224500524269 >> failure:<request_range:<key:\"/registry/masterleases/192.168.64.2\" > >"}
{"level":"info","ts":"2025-05-12T14:51:10.081773Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":671243}
{"level":"info","ts":"2025-05-12T14:51:10.089332Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":671243,"took":"6.85161ms","hash":765913290,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":3178496,"current-db-size-in-use":"3.2 MB"}
{"level":"info","ts":"2025-05-12T14:51:10.089682Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":765913290,"revision":671243,"compact-revision":670961}
{"level":"info","ts":"2025-05-12T14:56:10.099534Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":671701}
{"level":"info","ts":"2025-05-12T14:56:10.115567Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":671701,"took":"14.23101ms","hash":2712332436,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":3522560,"current-db-size-in-use":"3.5 MB"}
{"level":"info","ts":"2025-05-12T14:56:10.116201Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2712332436,"revision":671701,"compact-revision":671243}
{"level":"info","ts":"2025-05-12T15:01:10.122953Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":671977}
{"level":"info","ts":"2025-05-12T15:01:10.128379Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":671977,"took":"4.707588ms","hash":4024358660,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":2187264,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-05-12T15:01:10.128568Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4024358660,"revision":671977,"compact-revision":671701}
{"level":"info","ts":"2025-05-12T15:06:10.149417Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":672257}
{"level":"info","ts":"2025-05-12T15:06:10.156005Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":672257,"took":"6.163094ms","hash":3004988018,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":2162688,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-05-12T15:06:10.156512Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3004988018,"revision":672257,"compact-revision":671977}
{"level":"info","ts":"2025-05-12T15:10:49.788595Z","caller":"traceutil/trace.go:171","msg":"trace[1717620978] transaction","detail":"{read_only:false; response_revision:672793; number_of_response:1; }","duration":"109.436739ms","start":"2025-05-12T15:10:49.679007Z","end":"2025-05-12T15:10:49.788443Z","steps":["trace[1717620978] 'process raft request'  (duration: 107.723029ms)"],"step_count":1}
{"level":"info","ts":"2025-05-12T15:11:10.171748Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":672533}
{"level":"info","ts":"2025-05-12T15:11:10.179995Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":672533,"took":"6.612337ms","hash":2335879439,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":2162688,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-05-12T15:11:10.180295Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2335879439,"revision":672533,"compact-revision":672257}
{"level":"info","ts":"2025-05-12T15:16:10.225561Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":672813}
{"level":"info","ts":"2025-05-12T15:16:10.238790Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":672813,"took":"10.547811ms","hash":3124121961,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":2158592,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-05-12T15:16:10.239448Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3124121961,"revision":672813,"compact-revision":672533}
{"level":"info","ts":"2025-05-12T15:21:10.266925Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":673090}
{"level":"info","ts":"2025-05-12T15:21:10.272721Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":673090,"took":"4.077842ms","hash":580015087,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":2129920,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-05-12T15:21:10.272893Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":580015087,"revision":673090,"compact-revision":672813}
{"level":"info","ts":"2025-05-12T15:26:10.297909Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":673375}
{"level":"info","ts":"2025-05-12T15:26:10.304306Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":673375,"took":"5.323659ms","hash":1599145289,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":2179072,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-05-12T15:26:10.304500Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1599145289,"revision":673375,"compact-revision":673090}
{"level":"info","ts":"2025-05-12T15:31:10.324309Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":673653}
{"level":"info","ts":"2025-05-12T15:31:10.330507Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":673653,"took":"5.674855ms","hash":1311612955,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":2023424,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-05-12T15:31:10.330713Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1311612955,"revision":673653,"compact-revision":673375}
{"level":"info","ts":"2025-05-12T15:58:12.585816Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":673931}
{"level":"info","ts":"2025-05-12T15:58:12.591810Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":673931,"took":"4.544316ms","hash":3730035585,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":2019328,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-05-12T15:58:12.592654Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3730035585,"revision":673931,"compact-revision":673653}
{"level":"info","ts":"2025-05-12T16:02:22.834265Z","caller":"traceutil/trace.go:171","msg":"trace[1452340145] transaction","detail":"{read_only:false; response_revision:674449; number_of_response:1; }","duration":"140.048075ms","start":"2025-05-12T16:02:22.694113Z","end":"2025-05-12T16:02:22.834161Z","steps":["trace[1452340145] 'process raft request'  (duration: 138.62672ms)"],"step_count":1}
{"level":"info","ts":"2025-05-12T16:03:12.613218Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":674210}
{"level":"info","ts":"2025-05-12T16:03:12.618193Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":674210,"took":"4.036187ms","hash":2222358297,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":2195456,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-05-12T16:03:12.619347Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2222358297,"revision":674210,"compact-revision":673931}
{"level":"info","ts":"2025-05-12T16:08:12.633208Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":674495}
{"level":"info","ts":"2025-05-12T16:08:12.640727Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":674495,"took":"7.105653ms","hash":883837674,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":2207744,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-05-12T16:08:12.641074Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":883837674,"revision":674495,"compact-revision":674210}
{"level":"info","ts":"2025-05-12T16:09:09.734648Z","caller":"traceutil/trace.go:171","msg":"trace[942460014] transaction","detail":"{read_only:false; response_revision:674834; number_of_response:1; }","duration":"157.911666ms","start":"2025-05-12T16:09:09.576703Z","end":"2025-05-12T16:09:09.734615Z","steps":["trace[942460014] 'process raft request'  (duration: 157.539928ms)"],"step_count":1}
{"level":"info","ts":"2025-05-12T16:12:03.553135Z","caller":"traceutil/trace.go:171","msg":"trace[1001645752] linearizableReadLoop","detail":"{readStateIndex:806235; appliedIndex:806234; }","duration":"104.666803ms","start":"2025-05-12T16:12:03.448445Z","end":"2025-05-12T16:12:03.553112Z","steps":["trace[1001645752] 'read index received'  (duration: 104.060716ms)","trace[1001645752] 'applied index is now lower than readState.Index'  (duration: 583.436¬µs)"],"step_count":2}
{"level":"info","ts":"2025-05-12T16:12:03.554020Z","caller":"traceutil/trace.go:171","msg":"trace[746807132] transaction","detail":"{read_only:false; response_revision:675024; number_of_response:1; }","duration":"262.771315ms","start":"2025-05-12T16:12:03.291221Z","end":"2025-05-12T16:12:03.553993Z","steps":["trace[746807132] 'process raft request'  (duration: 261.438204ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-12T16:12:03.554402Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.962895ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2025-05-12T16:12:03.554465Z","caller":"traceutil/trace.go:171","msg":"trace[300300700] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:675024; }","duration":"106.053599ms","start":"2025-05-12T16:12:03.448387Z","end":"2025-05-12T16:12:03.554441Z","steps":["trace[300300700] 'agreement among raft nodes before linearized reading'  (duration: 105.886062ms)"],"step_count":1}
{"level":"info","ts":"2025-05-12T16:12:13.800371Z","caller":"traceutil/trace.go:171","msg":"trace[1653944576] transaction","detail":"{read_only:false; response_revision:675033; number_of_response:1; }","duration":"190.605305ms","start":"2025-05-12T16:12:13.609736Z","end":"2025-05-12T16:12:13.800341Z","steps":["trace[1653944576] 'process raft request'  (duration: 190.245122ms)"],"step_count":1}
{"level":"info","ts":"2025-05-12T16:13:12.648469Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":674774}
{"level":"info","ts":"2025-05-12T16:13:12.654716Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":674774,"took":"5.985592ms","hash":280341235,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":2711552,"current-db-size-in-use":"2.7 MB"}
{"level":"info","ts":"2025-05-12T16:13:12.655564Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":280341235,"revision":674774,"compact-revision":674495}
{"level":"info","ts":"2025-05-12T16:18:12.668413Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":675104}
{"level":"info","ts":"2025-05-12T16:18:12.687031Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":675104,"took":"15.453995ms","hash":98072705,"current-db-size-bytes":12025856,"current-db-size":"12 MB","current-db-size-in-use-bytes":2961408,"current-db-size-in-use":"3.0 MB"}
{"level":"info","ts":"2025-05-12T16:18:12.687128Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":98072705,"revision":675104,"compact-revision":674774}


==> kernel <==
 16:20:58 up  5:18,  0 users,  load average: 2.74, 1.58, 1.35
Linux minikube 5.10.207 #1 SMP Tue Jan 14 08:15:54 UTC 2025 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2023.02.9"


==> kube-apiserver [322ae144d369] <==
I0512 07:51:44.916590       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0512 07:51:44.999229       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0512 07:51:45.022569       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0512 07:51:45.881303       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0512 07:51:46.214287       1 controller.go:615] quota admission added evaluator for: endpoints
I0512 07:51:46.264876       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0512 07:52:04.025286       1 controller.go:615] quota admission added evaluator for: ingresses.networking.k8s.io
I0512 08:34:30.917633       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0512 08:34:30.947366       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.64.2]
E0512 08:53:55.347353       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0512 08:53:55.349460       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0512 08:53:58.797681       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0512 08:53:58.799011       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0512 08:53:58.893459       1 controller.go:615] quota admission added evaluator for: horizontalpodautoscalers.autoscaling
I0512 09:04:13.886273       1 alloc.go:330] "allocated clusterIPs" service="default/my-app-service" clusterIPs={"IPv4":"10.97.76.5"}
E0512 09:22:09.971255       1 conn.go:339] Error on socket receive: read tcp 192.168.64.2:8443->192.168.64.1:51229: use of closed network connection
I0512 09:28:24.994160       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0512 09:28:25.027063       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.64.2]
E0512 10:01:09.471085       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0512 10:01:09.473043       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0512 10:01:09.476265       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0512 10:01:09.481562       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="12.889652ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null
E0512 10:01:09.487460       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 82.659¬µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0512 12:46:33.074878       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0512 12:46:33.077612       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0512 12:46:34.955098       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0512 13:36:21.970648       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"context canceled\"}: context canceled" logger="UnhandledError"
E0512 13:36:22.233582       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0512 13:36:22.332117       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0512 13:36:22.335483       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0512 13:36:22.341604       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="360.408031ms" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0512 13:36:23.936318       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="39.675¬µs" method="POST" path="/api/v1/namespaces/default/events" result=null
E0512 13:36:23.979866       1 wrap.go:53] "Timeout or abort while handling" logger="UnhandledError" method="POST" URI="/api/v1/namespaces/default/events" auditID="fbb449fc-9b34-41c9-ac7f-429244455027"
E0512 13:36:24.762232       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0512 13:36:24.762619       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 70.957¬µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0512 13:36:24.770323       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0512 13:36:24.834443       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0512 13:36:24.915289       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="153.324969ms" method="PUT" path="/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube" result=null
E0512 13:36:24.927192       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 1.011021178s, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
I0512 14:11:44.336540       1 alloc.go:330] "allocated clusterIPs" service="default/my-app-service" clusterIPs={"IPv4":"10.100.39.250"}
I0512 14:43:41.196536       1 controller.go:615] quota admission added evaluator for: namespaces
I0512 14:46:24.782506       1 alloc.go:330] "allocated clusterIPs" service="monitoring/prometheus-server" clusterIPs={"IPv4":"10.101.4.159"}
I0512 14:46:24.955580       1 alloc.go:330] "allocated clusterIPs" service="monitoring/prometheus-alertmanager" clusterIPs={"IPv4":"10.110.37.202"}
I0512 14:46:25.045389       1 alloc.go:330] "allocated clusterIPs" service="monitoring/prometheus-prometheus-pushgateway" clusterIPs={"IPv4":"10.97.9.57"}
I0512 14:46:25.104863       1 alloc.go:330] "allocated clusterIPs" service="monitoring/prometheus-prometheus-node-exporter" clusterIPs={"IPv4":"10.101.128.40"}
I0512 14:46:25.168757       1 alloc.go:330] "allocated clusterIPs" service="monitoring/prometheus-kube-state-metrics" clusterIPs={"IPv4":"10.108.237.23"}
I0512 14:46:25.675572       1 controller.go:615] quota admission added evaluator for: statefulsets.apps
I0512 14:46:25.695163       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
E0512 16:01:18.439974       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0512 16:01:18.443548       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 37.214¬µs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E0512 16:01:18.445058       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0512 16:01:18.447136       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0512 16:01:18.452024       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="13.034009ms" method="POST" path="/apis/rbac.authorization.k8s.io/v1/clusterroles" result=null
I0512 16:09:22.841020       1 alloc.go:330] "allocated clusterIPs" service="monitoring/grafana" clusterIPs={"IPv4":"10.109.89.136"}
E0512 16:19:34.681529       1 upgradeaware.go:427] Error proxying data from client to backend: websocket: close 1006 (abnormal closure): unexpected EOF
E0512 16:19:34.682403       1 upgradeaware.go:441] Error proxying data from backend to client: websocket: close sent
E0512 16:20:07.261337       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0512 16:20:07.263052       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0512 16:20:25.285903       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0512 16:20:25.836023       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"


==> kube-apiserver [416cf5a9588b] <==
I0511 22:26:33.669246       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0511 22:26:33.669547       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0511 22:26:33.669967       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0511 22:26:33.670155       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0511 22:26:33.670409       1 aggregator.go:169] waiting for initial CRD sync...
I0511 22:26:33.676962       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0511 22:26:33.677139       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0511 22:26:33.677396       1 controller.go:78] Starting OpenAPI AggregationController
I0511 22:26:33.678195       1 controller.go:119] Starting legacy_token_tracking_controller
I0511 22:26:33.678358       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0511 22:26:33.686135       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0511 22:26:33.686717       1 local_available_controller.go:156] Starting LocalAvailability controller
I0511 22:26:33.686877       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0511 22:26:33.688108       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0511 22:26:33.689078       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0511 22:26:33.689972       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0511 22:26:33.690019       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0511 22:26:33.698866       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0511 22:26:33.699193       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0511 22:26:33.742255       1 controller.go:142] Starting OpenAPI controller
I0511 22:26:33.742945       1 controller.go:90] Starting OpenAPI V3 controller
I0511 22:26:33.743388       1 naming_controller.go:294] Starting NamingConditionController
I0511 22:26:33.743573       1 establishing_controller.go:81] Starting EstablishingController
I0511 22:26:33.743971       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0511 22:26:33.744891       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0511 22:26:33.745237       1 crd_finalizer.go:269] Starting CRDFinalizer
I0511 22:26:33.745935       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0511 22:26:33.746357       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0511 22:26:33.949608       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0511 22:26:33.949718       1 policy_source.go:240] refreshing policies
I0511 22:26:34.011001       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0511 22:26:34.027502       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0511 22:26:34.029917       1 aggregator.go:171] initial CRD sync complete...
I0511 22:26:34.029986       1 autoregister_controller.go:144] Starting autoregister controller
I0511 22:26:34.030017       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0511 22:26:34.030476       1 cache.go:39] Caches are synced for autoregister controller
I0511 22:26:34.051315       1 shared_informer.go:320] Caches are synced for configmaps
I0511 22:26:34.071027       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0511 22:26:34.083081       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0511 22:26:34.084007       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0511 22:26:34.084056       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0511 22:26:34.088157       1 cache.go:39] Caches are synced for LocalAvailability controller
I0511 22:26:34.098260       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0511 22:26:34.111977       1 shared_informer.go:320] Caches are synced for node_authorizer
I0511 22:26:34.130786       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
E0511 22:26:34.146041       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0511 22:26:34.705050       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0511 22:26:37.005968       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0511 22:26:37.053308       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0511 22:26:37.152576       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0511 22:26:37.336227       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0511 22:26:37.366031       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0511 22:26:53.042978       1 controller.go:615] quota admission added evaluator for: endpoints
I0511 22:26:53.047830       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0511 22:26:53.098096       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0511 22:27:01.640420       1 controller.go:615] quota admission added evaluator for: ingresses.networking.k8s.io
E0512 03:56:03.065642       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I0512 03:56:03.164903       1 controller.go:615] quota admission added evaluator for: horizontalpodautoscalers.autoscaling
E0512 03:56:04.816993       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0512 03:56:04.817984       1 authentication.go:74] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"


==> kube-controller-manager [b9615f199fdc] <==
E0512 04:32:08.692877       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:32:23.985041       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:32:24.004266       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:32:39.018736       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:32:39.027912       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:32:54.032797       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:32:54.040129       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:33:09.074863       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:33:09.075825       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:33:24.087800       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:33:24.088846       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:33:39.108147       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:33:39.110008       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:33:54.127210       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:33:54.134263       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:34:09.158099       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:34:09.159042       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:34:24.171047       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:34:24.188867       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:34:39.204991       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:34:39.205037       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:34:54.217448       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:34:54.217889       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:35:09.243332       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:35:09.243332       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:35:24.256786       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:35:24.258401       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:35:39.281157       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:35:39.281190       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:35:54.296350       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:35:54.299942       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:36:09.322928       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:36:09.322948       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:36:24.335574       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:36:24.341057       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:36:39.366217       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:36:39.367021       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:36:54.394158       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:36:54.402572       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
I0512 04:37:00.479675       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
E0512 04:37:09.415922       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:37:09.419876       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:37:24.434507       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:37:24.440129       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:37:39.459885       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:37:39.463044       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:37:54.478522       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:37:54.483883       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:38:09.508532       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:38:09.511070       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:38:24.521580       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:38:24.523359       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:38:43.958293       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:38:43.957952       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:38:58.978945       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:38:58.979968       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:39:14.085268       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 04:39:14.087582       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:39:29.098225       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 04:39:29.103061       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"


==> kube-controller-manager [fa4c0fae3dea] <==
E0512 16:13:51.587457       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:14:06.600205       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:14:06.605188       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:14:21.626456       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:14:21.630072       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:14:36.645807       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:14:36.658142       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:14:51.739832       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:14:51.741008       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:15:06.756817       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:15:06.757518       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:15:21.810674       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:15:21.813337       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:15:36.830876       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:15:36.832066       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:15:51.863066       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:15:51.864223       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:16:06.886521       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:16:06.886680       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:16:21.917161       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:16:21.918432       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:16:36.935622       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:16:36.938453       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:16:51.971856       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:16:51.983357       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
I0512 16:16:53.703564       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
E0512 16:17:06.985426       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:17:06.996175       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:17:22.040058       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:17:22.045783       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:17:37.055681       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:17:37.062319       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:17:52.089168       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:17:52.089119       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:18:07.112435       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:18:07.117110       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:18:22.141955       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:18:22.147221       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:18:37.157051       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:18:37.162343       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:18:52.177260       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:18:52.178230       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:19:07.187333       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:19:07.192266       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:19:22.217586       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:19:22.220089       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:19:37.231024       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:19:37.233837       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:19:52.255076       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:19:52.256167       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:20:07.345367       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: Unauthorized" logger="UnhandledError"
E0512 16:20:07.347893       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: Unauthorized" logger="UnhandledError"
E0512 16:20:22.384642       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:20:22.398050       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
I0512 16:20:25.288243       1 garbagecollector.go:789] "failed to discover preferred resources" logger="garbage-collector-controller" error="the server has asked for the client to provide credentials"
E0512 16:20:25.838275       1 resource_quota_controller.go:446] "Unhandled Error" err="failed to discover resources: the server has asked for the client to provide credentials" logger="UnhandledError"
E0512 16:20:37.403464       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:20:37.408001       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"
E0512 16:20:52.442295       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/web-app: deployments/scale.apps \"web-app\" not found" logger="UnhandledError"
E0512 16:20:52.444436       1 horizontal.go:275] "Unhandled Error" err="failed to query scale subresource for Deployment/default/webapp: deployments/scale.apps \"webapp\" not found" logger="UnhandledError"


==> kube-proxy [95d457e893f1] <==
I0512 07:51:45.422039       1 server_linux.go:66] "Using iptables proxy"
E0512 07:51:45.906203       1 proxier.go:733] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-24: Error: Could not process rule: Operation not supported
	add table ip kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^
 >
E0512 07:51:46.576675       1 proxier.go:733] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-25: Error: Could not process rule: Operation not supported
	add table ip6 kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^^
 >
I0512 07:51:46.760795       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.64.2"]
E0512 07:51:46.761007       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0512 07:51:46.877009       1 server_linux.go:147] "No iptables support for family" ipFamily="IPv6"
I0512 07:51:46.877094       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0512 07:51:46.877117       1 server_linux.go:170] "Using iptables Proxier"
I0512 07:51:46.888089       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0512 07:51:46.892310       1 server.go:497] "Version info" version="v1.32.0"
I0512 07:51:46.892454       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0512 07:51:46.900269       1 config.go:199] "Starting service config controller"
I0512 07:51:46.900478       1 config.go:105] "Starting endpoint slice config controller"
I0512 07:51:46.903171       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0512 07:51:46.903347       1 config.go:329] "Starting node config controller"
I0512 07:51:46.903503       1 shared_informer.go:313] Waiting for caches to sync for node config
I0512 07:51:46.903171       1 shared_informer.go:313] Waiting for caches to sync for service config
I0512 07:51:47.005061       1 shared_informer.go:320] Caches are synced for node config
I0512 07:51:47.005071       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0512 07:51:47.009271       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [db593fbae92c] <==
I0511 22:26:56.851537       1 server_linux.go:66] "Using iptables proxy"
E0511 22:26:57.048055       1 proxier.go:733] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-24: Error: Could not process rule: Operation not supported
	add table ip kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^
 >
E0511 22:26:57.111079       1 proxier.go:733] "Error cleaning up nftables rules" err=<
	could not run nftables command: /dev/stdin:1:1-25: Error: Could not process rule: Operation not supported
	add table ip6 kube-proxy
	^^^^^^^^^^^^^^^^^^^^^^^^^
 >
I0511 22:26:57.306584       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.64.2"]
E0511 22:26:57.307325       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0511 22:26:57.801443       1 server_linux.go:147] "No iptables support for family" ipFamily="IPv6"
I0511 22:26:57.801501       1 server.go:245] "kube-proxy running in single-stack mode" ipFamily="IPv4"
I0511 22:26:57.801550       1 server_linux.go:170] "Using iptables Proxier"
I0511 22:26:57.936488       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0511 22:26:57.940500       1 server.go:497] "Version info" version="v1.32.0"
I0511 22:26:57.940535       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0511 22:26:57.982131       1 config.go:199] "Starting service config controller"
I0511 22:26:57.983144       1 config.go:105] "Starting endpoint slice config controller"
I0511 22:26:57.983624       1 shared_informer.go:313] Waiting for caches to sync for service config
I0511 22:26:57.983983       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0511 22:26:57.984595       1 config.go:329] "Starting node config controller"
I0511 22:26:57.984660       1 shared_informer.go:313] Waiting for caches to sync for node config
I0511 22:26:58.085448       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0511 22:26:58.188929       1 shared_informer.go:320] Caches are synced for node config
I0511 22:26:58.189019       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [7f20e53f9eb0] <==
E0511 22:26:24.259903       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://192.168.64.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:24.279910       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: Get "https://192.168.64.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:24.280029       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get \"https://192.168.64.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:24.674458       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.64.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:24.674572       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.64.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:24.766870       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.64.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:24.766976       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://192.168.64.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:24.932520       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.64.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:24.932725       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://192.168.64.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:24.951156       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: Get "https://192.168.64.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:24.951252       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get \"https://192.168.64.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:25.011341       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: Get "https://192.168.64.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:25.011698       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get \"https://192.168.64.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:25.062126       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.64.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:25.062374       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get \"https://192.168.64.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:25.172500       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.64.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:25.173133       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get \"https://192.168.64.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:25.320709       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.64.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:25.321357       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get \"https://192.168.64.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:25.347361       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.64.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:25.348270       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://192.168.64.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:25.373907       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.64.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:25.374438       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://192.168.64.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:25.389024       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://192.168.64.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:25.389915       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://192.168.64.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:25.755733       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: Get "https://192.168.64.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:25.755960       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: Get \"https://192.168.64.2:8443/apis/storage.k8s.io/v1/volumeattachments?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:28.206056       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: Get "https://192.168.64.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:28.206208       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get \"https://192.168.64.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:28.356012       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.64.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:28.356897       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.64.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:28.628278       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: Get "https://192.168.64.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:28.628383       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get \"https://192.168.64.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:28.688564       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: Get "https://192.168.64.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:28.688929       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get \"https://192.168.64.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:28.690272       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: Get "https://192.168.64.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:28.690424       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get \"https://192.168.64.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:28.756118       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: Get "https://192.168.64.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:28.756203       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get \"https://192.168.64.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:28.781385       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: Get "https://192.168.64.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:28.781525       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get \"https://192.168.64.2:8443/api/v1/namespaces?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:29.223499       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: Get "https://192.168.64.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:29.223717       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: Get \"https://192.168.64.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:29.753229       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: Get "https://192.168.64.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:29.753416       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get \"https://192.168.64.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:30.067576       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://192.168.64.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:30.068041       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://192.168.64.2:8443/api/v1/nodes?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:30.473320       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://192.168.64.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:30.473406       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://192.168.64.2:8443/api/v1/services?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:30.659095       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: Get "https://192.168.64.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.64.2:8443: connect: connection refused
E0511 22:26:30.659598       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get \"https://192.168.64.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0\": dial tcp 192.168.64.2:8443: connect: connection refused" logger="UnhandledError"
W0511 22:26:33.755941       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0511 22:26:33.756336       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0511 22:26:33.757255       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0511 22:26:33.757627       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0511 22:26:33.758519       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0511 22:26:33.760925       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0511 22:26:33.759969       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0511 22:26:33.761569       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
I0511 22:26:41.424487       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [9250c505c92a] <==
I0512 07:51:36.682081       1 serving.go:386] Generated self-signed cert in-memory
W0512 07:51:42.283935       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0512 07:51:42.284171       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0512 07:51:42.284244       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0512 07:51:42.284259       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0512 07:51:42.338196       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0512 07:51:42.338400       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0512 07:51:42.342493       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0512 07:51:42.342896       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0512 07:51:42.344179       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0512 07:51:42.344688       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0512 07:51:42.446307       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
May 12 16:09:03 minikube kubelet[1476]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 12 16:09:23 minikube kubelet[1476]: I0512 16:09:23.277706    1476 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config\" (UniqueName: \"kubernetes.io/configmap/19d58422-9ac7-43cf-858e-660fd4a5b1bc-config\") pod \"grafana-db4c5c8f6-jmpl7\" (UID: \"19d58422-9ac7-43cf-858e-660fd4a5b1bc\") " pod="monitoring/grafana-db4c5c8f6-jmpl7"
May 12 16:09:23 minikube kubelet[1476]: I0512 16:09:23.278225    1476 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"storage\" (UniqueName: \"kubernetes.io/empty-dir/19d58422-9ac7-43cf-858e-660fd4a5b1bc-storage\") pod \"grafana-db4c5c8f6-jmpl7\" (UID: \"19d58422-9ac7-43cf-858e-660fd4a5b1bc\") " pod="monitoring/grafana-db4c5c8f6-jmpl7"
May 12 16:09:23 minikube kubelet[1476]: I0512 16:09:23.278321    1476 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-m87qb\" (UniqueName: \"kubernetes.io/projected/19d58422-9ac7-43cf-858e-660fd4a5b1bc-kube-api-access-m87qb\") pod \"grafana-db4c5c8f6-jmpl7\" (UID: \"19d58422-9ac7-43cf-858e-660fd4a5b1bc\") " pod="monitoring/grafana-db4c5c8f6-jmpl7"
May 12 16:10:03 minikube kubelet[1476]: E0512 16:10:03.777840    1476 iptables.go:577] "Could not set up iptables canary" err=<
May 12 16:10:03 minikube kubelet[1476]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 12 16:10:03 minikube kubelet[1476]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 12 16:10:03 minikube kubelet[1476]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 12 16:10:03 minikube kubelet[1476]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 12 16:11:03 minikube kubelet[1476]: E0512 16:11:03.771217    1476 iptables.go:577] "Could not set up iptables canary" err=<
May 12 16:11:03 minikube kubelet[1476]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 12 16:11:03 minikube kubelet[1476]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 12 16:11:03 minikube kubelet[1476]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 12 16:11:03 minikube kubelet[1476]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 12 16:12:03 minikube kubelet[1476]: E0512 16:12:03.787874    1476 iptables.go:577] "Could not set up iptables canary" err=<
May 12 16:12:03 minikube kubelet[1476]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 12 16:12:03 minikube kubelet[1476]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 12 16:12:03 minikube kubelet[1476]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 12 16:12:03 minikube kubelet[1476]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 12 16:12:37 minikube kubelet[1476]: I0512 16:12:37.139392    1476 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="monitoring/grafana-db4c5c8f6-jmpl7" podStartSLOduration=14.948782425 podStartE2EDuration="3m14.139357925s" podCreationTimestamp="2025-05-12 16:09:23 +0000 UTC" firstStartedPulling="2025-05-12 16:09:24.20561271 +0000 UTC m=+18380.863010761" lastFinishedPulling="2025-05-12 16:12:23.396187415 +0000 UTC m=+18560.053586261" observedRunningTime="2025-05-12 16:12:25.063035719 +0000 UTC m=+18561.720434765" watchObservedRunningTime="2025-05-12 16:12:37.139357925 +0000 UTC m=+18573.796757373"
May 12 16:13:03 minikube kubelet[1476]: E0512 16:13:03.776416    1476 iptables.go:577] "Could not set up iptables canary" err=<
May 12 16:13:03 minikube kubelet[1476]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 12 16:13:03 minikube kubelet[1476]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 12 16:13:03 minikube kubelet[1476]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 12 16:13:03 minikube kubelet[1476]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 12 16:14:03 minikube kubelet[1476]: E0512 16:14:03.784145    1476 iptables.go:577] "Could not set up iptables canary" err=<
May 12 16:14:03 minikube kubelet[1476]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 12 16:14:03 minikube kubelet[1476]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 12 16:14:03 minikube kubelet[1476]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 12 16:14:03 minikube kubelet[1476]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 12 16:15:03 minikube kubelet[1476]: E0512 16:15:03.778778    1476 iptables.go:577] "Could not set up iptables canary" err=<
May 12 16:15:03 minikube kubelet[1476]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 12 16:15:03 minikube kubelet[1476]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 12 16:15:03 minikube kubelet[1476]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 12 16:15:03 minikube kubelet[1476]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 12 16:16:03 minikube kubelet[1476]: E0512 16:16:03.770763    1476 iptables.go:577] "Could not set up iptables canary" err=<
May 12 16:16:03 minikube kubelet[1476]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 12 16:16:03 minikube kubelet[1476]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 12 16:16:03 minikube kubelet[1476]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 12 16:16:03 minikube kubelet[1476]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 12 16:17:03 minikube kubelet[1476]: E0512 16:17:03.772669    1476 iptables.go:577] "Could not set up iptables canary" err=<
May 12 16:17:03 minikube kubelet[1476]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 12 16:17:03 minikube kubelet[1476]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 12 16:17:03 minikube kubelet[1476]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 12 16:17:03 minikube kubelet[1476]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 12 16:18:03 minikube kubelet[1476]: E0512 16:18:03.778310    1476 iptables.go:577] "Could not set up iptables canary" err=<
May 12 16:18:03 minikube kubelet[1476]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 12 16:18:03 minikube kubelet[1476]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 12 16:18:03 minikube kubelet[1476]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 12 16:18:03 minikube kubelet[1476]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 12 16:19:03 minikube kubelet[1476]: E0512 16:19:03.768505    1476 iptables.go:577] "Could not set up iptables canary" err=<
May 12 16:19:03 minikube kubelet[1476]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 12 16:19:03 minikube kubelet[1476]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 12 16:19:03 minikube kubelet[1476]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 12 16:19:03 minikube kubelet[1476]:  > table="nat" chain="KUBE-KUBELET-CANARY"
May 12 16:20:03 minikube kubelet[1476]: E0512 16:20:03.771023    1476 iptables.go:577] "Could not set up iptables canary" err=<
May 12 16:20:03 minikube kubelet[1476]:         error creating chain "KUBE-KUBELET-CANARY": exit status 3: Ignoring deprecated --wait-interval option.
May 12 16:20:03 minikube kubelet[1476]:         ip6tables v1.8.9 (legacy): can't initialize ip6tables table `nat': Table does not exist (do you need to insmod?)
May 12 16:20:03 minikube kubelet[1476]:         Perhaps ip6tables or your kernel needs to be upgraded.
May 12 16:20:03 minikube kubelet[1476]:  > table="nat" chain="KUBE-KUBELET-CANARY"


==> storage-provisioner [0dc922fac251] <==
I0512 13:36:31.643867       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0512 13:36:32.126399       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0512 13:36:32.126494       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0512 13:36:51.407052       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0512 13:36:51.415849       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_7dd6060e-e748-4c45-a939-4a0a0dc29fd4!
I0512 13:36:51.467694       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"9cc4c2cd-237a-4e63-b6dd-920797851a3c", APIVersion:"v1", ResourceVersion:"667375", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_7dd6060e-e748-4c45-a939-4a0a0dc29fd4 became leader
I0512 13:36:54.614685       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_7dd6060e-e748-4c45-a939-4a0a0dc29fd4!
I0512 14:20:30.907425       1 controller.go:1332] provision "default/my-app-pvc" class "standard": started
I0512 14:20:30.919869       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"my-app-pvc", UID:"db28d02e-d111-4b01-a45b-f7cbea0f61b6", APIVersion:"v1", ResourceVersion:"669808", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "default/my-app-pvc"
I0512 14:20:30.911509       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    bed8682b-329a-4c1c-9be3-7ab7ea91dad3 317 0 2025-03-23 05:10:58 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2025-03-23 05:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-db28d02e-d111-4b01-a45b-f7cbea0f61b6 &PersistentVolumeClaim{ObjectMeta:{my-app-pvc  default  db28d02e-d111-4b01-a45b-f7cbea0f61b6 669808 0 2025-05-12 14:20:30 +0000 UTC <nil> <nil> map[] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"my-app-pvc","namespace":"default"},"spec":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"1Gi"}}}}
 volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2025-05-12 14:20:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}}}}} {kubectl-client-side-apply Update v1 2025-05-12 14:20:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{1073741824 0} {<nil>} 1Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/default/my-app-pvc
I0512 14:20:30.938865       1 controller.go:1439] provision "default/my-app-pvc" class "standard": volume "pvc-db28d02e-d111-4b01-a45b-f7cbea0f61b6" provisioned
I0512 14:20:30.945228       1 controller.go:1456] provision "default/my-app-pvc" class "standard": succeeded
I0512 14:20:30.945417       1 volume_store.go:212] Trying to save persistentvolume "pvc-db28d02e-d111-4b01-a45b-f7cbea0f61b6"
I0512 14:20:30.994212       1 volume_store.go:219] persistentvolume "pvc-db28d02e-d111-4b01-a45b-f7cbea0f61b6" saved
I0512 14:20:30.996030       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"default", Name:"my-app-pvc", UID:"db28d02e-d111-4b01-a45b-f7cbea0f61b6", APIVersion:"v1", ResourceVersion:"669808", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-db28d02e-d111-4b01-a45b-f7cbea0f61b6
I0512 14:46:24.315185       1 controller.go:1332] provision "monitoring/prometheus-server" class "standard": started
I0512 14:46:24.323962       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    bed8682b-329a-4c1c-9be3-7ab7ea91dad3 317 0 2025-03-23 05:10:58 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2025-03-23 05:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-399b8c80-d960-4aa3-9851-e41f9191068a &PersistentVolumeClaim{ObjectMeta:{prometheus-server  monitoring  399b8c80-d960-4aa3-9851-e41f9191068a 671264 0 2025-05-12 14:46:24 +0000 UTC <nil> <nil> map[app.kubernetes.io/component:server app.kubernetes.io/instance:prometheus app.kubernetes.io/managed-by:Helm app.kubernetes.io/name:prometheus app.kubernetes.io/part-of:prometheus app.kubernetes.io/version:v3.3.1 helm.sh/chart:prometheus-27.13.0] map[meta.helm.sh/release-name:prometheus meta.helm.sh/release-namespace:monitoring volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{helm Update v1 2025-05-12 14:46:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:meta.helm.sh/release-name":{},"f:meta.helm.sh/release-namespace":{}},"f:labels":{".":{},"f:app.kubernetes.io/component":{},"f:app.kubernetes.io/instance":{},"f:app.kubernetes.io/managed-by":{},"f:app.kubernetes.io/name":{},"f:app.kubernetes.io/part-of":{},"f:app.kubernetes.io/version":{},"f:helm.sh/chart":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}} {kube-controller-manager Update v1 2025-05-12 14:46:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{8589934592 0} {<nil>}  BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/monitoring/prometheus-server
I0512 14:46:24.335074       1 controller.go:1439] provision "monitoring/prometheus-server" class "standard": volume "pvc-399b8c80-d960-4aa3-9851-e41f9191068a" provisioned
I0512 14:46:24.341841       1 controller.go:1456] provision "monitoring/prometheus-server" class "standard": succeeded
I0512 14:46:24.342166       1 volume_store.go:212] Trying to save persistentvolume "pvc-399b8c80-d960-4aa3-9851-e41f9191068a"
I0512 14:46:24.371079       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"monitoring", Name:"prometheus-server", UID:"399b8c80-d960-4aa3-9851-e41f9191068a", APIVersion:"v1", ResourceVersion:"671264", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "monitoring/prometheus-server"
I0512 14:46:24.492727       1 volume_store.go:219] persistentvolume "pvc-399b8c80-d960-4aa3-9851-e41f9191068a" saved
I0512 14:46:24.497122       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"monitoring", Name:"prometheus-server", UID:"399b8c80-d960-4aa3-9851-e41f9191068a", APIVersion:"v1", ResourceVersion:"671264", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-399b8c80-d960-4aa3-9851-e41f9191068a
I0512 14:46:28.858011       1 controller.go:1332] provision "monitoring/storage-prometheus-alertmanager-0" class "standard": started
I0512 14:46:28.858302       1 storage_provisioner.go:61] Provisioning volume {&StorageClass{ObjectMeta:{standard    bed8682b-329a-4c1c-9be3-7ab7ea91dad3 317 0 2025-03-23 05:10:58 +0000 UTC <nil> <nil> map[addonmanager.kubernetes.io/mode:EnsureExists] map[kubectl.kubernetes.io/last-applied-configuration:{"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"},"labels":{"addonmanager.kubernetes.io/mode":"EnsureExists"},"name":"standard"},"provisioner":"k8s.io/minikube-hostpath"}
 storageclass.kubernetes.io/is-default-class:true] [] []  [{kubectl-client-side-apply Update storage.k8s.io/v1 2025-03-23 05:10:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:kubectl.kubernetes.io/last-applied-configuration":{},"f:storageclass.kubernetes.io/is-default-class":{}},"f:labels":{".":{},"f:addonmanager.kubernetes.io/mode":{}}},"f:provisioner":{},"f:reclaimPolicy":{},"f:volumeBindingMode":{}}}]},Provisioner:k8s.io/minikube-hostpath,Parameters:map[string]string{},ReclaimPolicy:*Delete,MountOptions:[],AllowVolumeExpansion:nil,VolumeBindingMode:*Immediate,AllowedTopologies:[]TopologySelectorTerm{},} pvc-c42e8dbe-6576-4fba-b706-9a9b1b9a35f4 &PersistentVolumeClaim{ObjectMeta:{storage-prometheus-alertmanager-0  monitoring  c42e8dbe-6576-4fba-b706-9a9b1b9a35f4 671354 0 2025-05-12 14:46:28 +0000 UTC <nil> <nil> map[app.kubernetes.io/instance:prometheus app.kubernetes.io/name:alertmanager] map[volume.beta.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath volume.kubernetes.io/storage-provisioner:k8s.io/minikube-hostpath] [] [kubernetes.io/pvc-protection]  [{kube-controller-manager Update v1 2025-05-12 14:46:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volume.beta.kubernetes.io/storage-provisioner":{},"f:volume.kubernetes.io/storage-provisioner":{}},"f:labels":{".":{},"f:app.kubernetes.io/instance":{},"f:app.kubernetes.io/name":{}}},"f:spec":{"f:accessModes":{},"f:resources":{"f:requests":{".":{},"f:storage":{}}},"f:volumeMode":{}}}}]},Spec:PersistentVolumeClaimSpec{AccessModes:[ReadWriteOnce],Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{storage: {{2147483648 0} {<nil>} 2Gi BinarySI},},},VolumeName:,Selector:nil,StorageClassName:*standard,VolumeMode:*Filesystem,DataSource:nil,},Status:PersistentVolumeClaimStatus{Phase:Pending,AccessModes:[],Capacity:ResourceList{},Conditions:[]PersistentVolumeClaimCondition{},},} nil} to /tmp/hostpath-provisioner/monitoring/storage-prometheus-alertmanager-0
I0512 14:46:28.859908       1 controller.go:1439] provision "monitoring/storage-prometheus-alertmanager-0" class "standard": volume "pvc-c42e8dbe-6576-4fba-b706-9a9b1b9a35f4" provisioned
I0512 14:46:28.859957       1 controller.go:1456] provision "monitoring/storage-prometheus-alertmanager-0" class "standard": succeeded
I0512 14:46:28.860060       1 volume_store.go:212] Trying to save persistentvolume "pvc-c42e8dbe-6576-4fba-b706-9a9b1b9a35f4"
I0512 14:46:28.886179       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"monitoring", Name:"storage-prometheus-alertmanager-0", UID:"c42e8dbe-6576-4fba-b706-9a9b1b9a35f4", APIVersion:"v1", ResourceVersion:"671354", FieldPath:""}): type: 'Normal' reason: 'Provisioning' External provisioner is provisioning volume for claim "monitoring/storage-prometheus-alertmanager-0"
I0512 14:46:29.301502       1 volume_store.go:219] persistentvolume "pvc-c42e8dbe-6576-4fba-b706-9a9b1b9a35f4" saved
I0512 14:46:30.017195       1 event.go:282] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"monitoring", Name:"storage-prometheus-alertmanager-0", UID:"c42e8dbe-6576-4fba-b706-9a9b1b9a35f4", APIVersion:"v1", ResourceVersion:"671354", FieldPath:""}): type: 'Normal' reason: 'ProvisioningSucceeded' Successfully provisioned volume pvc-c42e8dbe-6576-4fba-b706-9a9b1b9a35f4


==> storage-provisioner [3f366d2bda14] <==
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc000138a00, 0x0, 0x18bc168, 0xc00027a6c0, 0x0, 0x0, 0x461dc0, 0xc0000dad20, 0xc000647e50)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc0002f2ec0, 0xc000647ef0, 0x8, 0x18baa48, 0xc0002fc000, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc00027b6c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

goroutine 15769 [sync.Cond.Wait, 1 minutes]:
sync.runtime_notifyListWait(0xc00016e1a0, 0xc000000000)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc00016e190)
	/usr/local/go/src/sync/cond.go:56 +0x99
golang.org/x/net/http2.(*pipe).Read(0xc00016e188, 0xc00065de00, 0x200, 0x200, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x97
golang.org/x/net/http2.transportResponseBody.Read(0xc00016e160, 0xc00065de00, 0x200, 0x200, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0xaf
encoding/json.(*Decoder).refill(0xc0001d38c0, 0x62616c3a6622, 0x7f9e5e0cedf0)
	/usr/local/go/src/encoding/json/stream.go:165 +0xeb
encoding/json.(*Decoder).readValue(0xc0001d38c0, 0x0, 0x0, 0x152aee0)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1ff
encoding/json.(*Decoder).Decode(0xc0001d38c0, 0x154a160, 0xc000301710, 0x203000, 0x203000)
	/usr/local/go/src/encoding/json/stream.go:63 +0x7c
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0xc0001d6de0, 0xc0000df400, 0x400, 0x400, 0x40, 0x38, 0x15b0440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x1a8
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc000080c30, 0x0, 0x18bc168, 0xc00045b300, 0x0, 0x0, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc0002f3dc0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc00045b2c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

goroutine 15248 [sync.Cond.Wait, 1 minutes]:
sync.runtime_notifyListWait(0xc000618880, 0x6)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0xc000618870)
	/usr/local/go/src/sync/cond.go:56 +0x99
golang.org/x/net/http2.(*pipe).Read(0xc000618868, 0xc0000fb201, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/pipe.go:65 +0x97
golang.org/x/net/http2.transportResponseBody.Read(0xc000618840, 0xc0000fb201, 0x5ff, 0x5ff, 0x0, 0x0, 0x0)
	/Users/medya/go/pkg/mod/golang.org/x/net@v0.0.0-20201224014010-6772e930b67b/http2/transport.go:2108 +0xaf
encoding/json.(*Decoder).refill(0xc0005a5ce0, 0xa, 0x9)
	/usr/local/go/src/encoding/json/stream.go:165 +0xeb
encoding/json.(*Decoder).readValue(0xc0005a5ce0, 0x0, 0x0, 0x152aee0)
	/usr/local/go/src/encoding/json/stream.go:140 +0x1ff
encoding/json.(*Decoder).Decode(0xc0005a5ce0, 0x154a160, 0xc000301020, 0x203000, 0x203000)
	/usr/local/go/src/encoding/json/stream.go:63 +0x7c
k8s.io/apimachinery/pkg/util/framer.(*jsonFrameReader).Read(0xc00008baa0, 0xc000405400, 0x400, 0x400, 0x40, 0x38, 0x15b0440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/framer/framer.go:152 +0x1a8
k8s.io/apimachinery/pkg/runtime/serializer/streaming.(*decoder).Decode(0xc000606550, 0x0, 0x18bc168, 0xc00045ae40, 0x0, 0x0, 0x461dc0, 0xc000101140, 0xc000123e50)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/runtime/serializer/streaming/streaming.go:77 +0x89
k8s.io/client-go/rest/watch.(*Decoder).Decode(0xc000310280, 0xc000123ef0, 0x8, 0x18bbba0, 0xc00060db00, 0x0, 0x0)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/rest/watch/decoder.go:49 +0x6e
k8s.io/apimachinery/pkg/watch.(*StreamWatcher).receive(0xc00045a440)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:104 +0x14a
created by k8s.io/apimachinery/pkg/watch.NewStreamWatcher
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/watch/streamwatcher.go:71 +0xbe

